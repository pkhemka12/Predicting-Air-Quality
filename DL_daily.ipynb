{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21188,"status":"ok","timestamp":1670389886296,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"bk9W2RCZaJtY","outputId":"12d26a4b-134a-473b-9edf-5989478a3b2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":567,"status":"ok","timestamp":1670389886860,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"TnOCaB6HaLlB","outputId":"92588f86-ddef-41b0-90c0-907559921b3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks/stats207\n"]}],"source":["%cd /content/drive/My Drive/Colab Notebooks/stats207"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QeMP4tGQZt9g"},"outputs":[],"source":["import numpy as np \n","import pandas as pd\n","import torch\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader \n","import sklearn.metrics as metrics\n","from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler,  FunctionTransformer"]},{"cell_type":"markdown","metadata":{"id":"hsAb14ixcXcu"},"source":["cites:\n"]},{"cell_type":"markdown","metadata":{"id":"Jndg53XDdDmr"},"source":["# Data Pre-Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1670389894535,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"GvVikXkCdIKl","outputId":"ea982127-b535-488a-e5af-dca7e2a27c96"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["         Date  Overall.AQI.Value Main.Pollutant Site.Name..of.Overall.AQI.  \\\n","0  2012-01-01                 83          PM2.5         San Jose - Jackson   \n","1  2012-01-02                 75          PM2.5         San Jose - Jackson   \n","2  2012-01-03                 93          PM2.5         San Jose - Jackson   \n","3  2012-01-04                 97          PM2.5         San Jose - Jackson   \n","4  2012-01-05                 65          PM2.5         San Jose - Jackson   \n","\n","   CO  Ozone  PM25  NO2  \n","0  14     22    83   31  \n","1  16     19    75   35  \n","2  17      8    93   33  \n","3  20     19    97   42  \n","4  20     18    65   48  "],"text/html":["\n","  <div id=\"df-72bd111b-1e75-4dd1-9ac9-28817dc16707\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Overall.AQI.Value</th>\n","      <th>Main.Pollutant</th>\n","      <th>Site.Name..of.Overall.AQI.</th>\n","      <th>CO</th>\n","      <th>Ozone</th>\n","      <th>PM25</th>\n","      <th>NO2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2012-01-01</td>\n","      <td>83</td>\n","      <td>PM2.5</td>\n","      <td>San Jose - Jackson</td>\n","      <td>14</td>\n","      <td>22</td>\n","      <td>83</td>\n","      <td>31</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2012-01-02</td>\n","      <td>75</td>\n","      <td>PM2.5</td>\n","      <td>San Jose - Jackson</td>\n","      <td>16</td>\n","      <td>19</td>\n","      <td>75</td>\n","      <td>35</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2012-01-03</td>\n","      <td>93</td>\n","      <td>PM2.5</td>\n","      <td>San Jose - Jackson</td>\n","      <td>17</td>\n","      <td>8</td>\n","      <td>93</td>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2012-01-04</td>\n","      <td>97</td>\n","      <td>PM2.5</td>\n","      <td>San Jose - Jackson</td>\n","      <td>20</td>\n","      <td>19</td>\n","      <td>97</td>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2012-01-05</td>\n","      <td>65</td>\n","      <td>PM2.5</td>\n","      <td>San Jose - Jackson</td>\n","      <td>20</td>\n","      <td>18</td>\n","      <td>65</td>\n","      <td>48</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72bd111b-1e75-4dd1-9ac9-28817dc16707')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-72bd111b-1e75-4dd1-9ac9-28817dc16707 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-72bd111b-1e75-4dd1-9ac9-28817dc16707');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}],"source":["aqi_daily = pd.read_csv('aqi_daily_cleaned.csv')\n","aqi_daily.head()"]},{"cell_type":"code","source":["aqi_daily.tail()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kcg2IUCQNvkU","executionInfo":{"status":"ok","timestamp":1670389894535,"user_tz":480,"elapsed":15,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}},"outputId":"82f3e5fb-7de2-4b6a-81b6-0a5ea95708d4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["            Date  Overall.AQI.Value Main.Pollutant Site.Name..of.Overall.AQI.  \\\n","3648  2021-12-27                 33          Ozone                 San Martin   \n","3649  2021-12-28                 32          Ozone                 San Martin   \n","3650  2021-12-29                 27          Ozone                 San Martin   \n","3651  2021-12-30                 24            NO2     San Jose - Knox Avenue   \n","3652  2021-12-31                 50          PM2.5     San Jose - Knox Avenue   \n","\n","      CO  Ozone  PM25  NO2  \n","3648   6     33    17   23  \n","3649   6     32    20   24  \n","3650   7     27    17   25  \n","3651   6     22    23   24  \n","3652   9     28    50   24  "],"text/html":["\n","  <div id=\"df-945f1b5f-ac65-4aa9-b81d-20105fb3f5e0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Overall.AQI.Value</th>\n","      <th>Main.Pollutant</th>\n","      <th>Site.Name..of.Overall.AQI.</th>\n","      <th>CO</th>\n","      <th>Ozone</th>\n","      <th>PM25</th>\n","      <th>NO2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3648</th>\n","      <td>2021-12-27</td>\n","      <td>33</td>\n","      <td>Ozone</td>\n","      <td>San Martin</td>\n","      <td>6</td>\n","      <td>33</td>\n","      <td>17</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>3649</th>\n","      <td>2021-12-28</td>\n","      <td>32</td>\n","      <td>Ozone</td>\n","      <td>San Martin</td>\n","      <td>6</td>\n","      <td>32</td>\n","      <td>20</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>3650</th>\n","      <td>2021-12-29</td>\n","      <td>27</td>\n","      <td>Ozone</td>\n","      <td>San Martin</td>\n","      <td>7</td>\n","      <td>27</td>\n","      <td>17</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>3651</th>\n","      <td>2021-12-30</td>\n","      <td>24</td>\n","      <td>NO2</td>\n","      <td>San Jose - Knox Avenue</td>\n","      <td>6</td>\n","      <td>22</td>\n","      <td>23</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>3652</th>\n","      <td>2021-12-31</td>\n","      <td>50</td>\n","      <td>PM2.5</td>\n","      <td>San Jose - Knox Avenue</td>\n","      <td>9</td>\n","      <td>28</td>\n","      <td>50</td>\n","      <td>24</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-945f1b5f-ac65-4aa9-b81d-20105fb3f5e0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-945f1b5f-ac65-4aa9-b81d-20105fb3f5e0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-945f1b5f-ac65-4aa9-b81d-20105fb3f5e0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"B86ZQi0fRpj4"},"source":["### Time Feature Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASOkw4fOTmlt"},"outputs":[],"source":["aqi_daily['Date'] = pd.to_datetime(aqi_daily['Date'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4c_ZkLcTDcc"},"outputs":[],"source":["aqi_daily['day_of_month'] = aqi_daily.Date.dt.day - 1 # ensure indexing starts from zero \n","aqi_daily['month'] = aqi_daily.Date.dt.month - 1\n","aqi_daily['day_of_week'] = aqi_daily.Date.dt.weekday"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4MES9pDRocy"},"outputs":[],"source":["# Sine/cosine transformation \n","def sin_transformer(period):\n","\treturn FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n","\n","def cos_transformer(period):\n","\treturn FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7wG5npr5S7_V"},"outputs":[],"source":["aqi_daily[\"day_of_month_sin\"] = sin_transformer(12).fit_transform(aqi_daily['day_of_month'])\n","aqi_daily[\"day_of_month_cos\"] = cos_transformer(12).fit_transform(aqi_daily['day_of_month'])\n","\n","aqi_daily[\"month_sin\"] = sin_transformer(12).fit_transform(aqi_daily['month'])\n","aqi_daily[\"month_cos\"] = cos_transformer(12).fit_transform(aqi_daily['month'])\n","\n","aqi_daily[\"day_of_week_sin\"] = sin_transformer(12).fit_transform(aqi_daily['day_of_week'])\n","aqi_daily[\"day_of_week_cos\"] = cos_transformer(12).fit_transform(aqi_daily['day_of_week'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trXBLAz-V0qd"},"outputs":[],"source":["aqi_daily_subset = aqi_daily.drop(['Date',\n"," 'Main.Pollutant',\n"," 'Site.Name..of.Overall.AQI.',\n"," 'day_of_month',\n"," 'month',\n"," 'day_of_week'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1670389894536,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"0ZlZwkJDdPGh","outputId":"c4351ad4-e1a8-4c3d-af8a-0e0e304ea7d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["(2922, 11)\n","(731, 11)\n"]}],"source":["# Split Train and Test \n","train = aqi_daily_subset.iloc[0:2922, :].values\n","test = aqi_daily_subset.iloc[2922:, :].values\n","\n","print(train.shape)\n","print(test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1670389894536,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"ay_amtccjjll","outputId":"11171712-66f8-4d0d-dbd2-0ab059f71d5e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 8.3000000e+01,  1.4000000e+01,  2.2000000e+01,  8.3000000e+01,\n","        3.1000000e+01,  0.0000000e+00,  1.0000000e+00,  0.0000000e+00,\n","        1.0000000e+00,  1.2246468e-16, -1.0000000e+00])"]},"metadata":{},"execution_count":12}],"source":["train[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOtczAeXdTTV"},"outputs":[],"source":["# Normalize Input \n","scaler = MinMaxScaler()\n","# scaler = StandardScaler() \n","\n","train[:, :5] = scaler.fit_transform(train[:, :5])\n","test[:, :5] = scaler.fit_transform(test[:, :5] )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1670389894537,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"80ZHm20-uvJn","outputId":"78e083ad-382b-4e0a-8327-195016ec02be"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 3.90109890e-01,  4.61538462e-01,  1.20879121e-01,  4.18848168e-01,\n","        3.25301205e-01,  0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n","        1.00000000e+00,  1.22464680e-16, -1.00000000e+00])"]},"metadata":{},"execution_count":14}],"source":["train[0]"]},{"cell_type":"markdown","metadata":{"id":"EOxahcgLaF4i"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZSmymBb3pT3Z"},"outputs":[],"source":["timesteps = 3\n","n_cols = 11"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9Tw_8qYre76"},"outputs":[],"source":["def create_sequences(data, timesteps, n_cols):\n","  data_timesteps = np.array([[j for j in data[i:i+timesteps+1]] for i in range(0,len(data)-timesteps)])[:,:,]\n","  x, y = data_timesteps[:,:-1, :], data_timesteps[:,-1:, 0]\n","\n","  return x, y "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lfZAY-jzGNw"},"outputs":[],"source":["x_train, y_train = create_sequences(train, 3, 11)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134,"status":"ok","timestamp":1670389894660,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"57m7ISG1X9ZQ","outputId":"1c07c170-2d41-4ae6-d458-89f163f13a04"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[ 3.90109890e-01,  4.61538462e-01,  1.20879121e-01,\n","          4.18848168e-01,  3.25301205e-01,  0.00000000e+00,\n","          1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n","          1.22464680e-16, -1.00000000e+00],\n","        [ 3.46153846e-01,  5.38461538e-01,  1.04395604e-01,\n","          3.76963351e-01,  3.73493976e-01,  5.00000000e-01,\n","          8.66025404e-01,  0.00000000e+00,  1.00000000e+00,\n","          0.00000000e+00,  1.00000000e+00],\n","        [ 4.45054945e-01,  5.76923077e-01,  4.39560440e-02,\n","          4.71204188e-01,  3.49397590e-01,  8.66025404e-01,\n","          5.00000000e-01,  0.00000000e+00,  1.00000000e+00,\n","          5.00000000e-01,  8.66025404e-01]],\n","\n","       [[ 3.46153846e-01,  5.38461538e-01,  1.04395604e-01,\n","          3.76963351e-01,  3.73493976e-01,  5.00000000e-01,\n","          8.66025404e-01,  0.00000000e+00,  1.00000000e+00,\n","          0.00000000e+00,  1.00000000e+00],\n","        [ 4.45054945e-01,  5.76923077e-01,  4.39560440e-02,\n","          4.71204188e-01,  3.49397590e-01,  8.66025404e-01,\n","          5.00000000e-01,  0.00000000e+00,  1.00000000e+00,\n","          5.00000000e-01,  8.66025404e-01],\n","        [ 4.67032967e-01,  6.92307692e-01,  1.04395604e-01,\n","          4.92146597e-01,  4.57831325e-01,  1.00000000e+00,\n","          6.12323400e-17,  0.00000000e+00,  1.00000000e+00,\n","          8.66025404e-01,  5.00000000e-01]],\n","\n","       [[ 4.45054945e-01,  5.76923077e-01,  4.39560440e-02,\n","          4.71204188e-01,  3.49397590e-01,  8.66025404e-01,\n","          5.00000000e-01,  0.00000000e+00,  1.00000000e+00,\n","          5.00000000e-01,  8.66025404e-01],\n","        [ 4.67032967e-01,  6.92307692e-01,  1.04395604e-01,\n","          4.92146597e-01,  4.57831325e-01,  1.00000000e+00,\n","          6.12323400e-17,  0.00000000e+00,  1.00000000e+00,\n","          8.66025404e-01,  5.00000000e-01],\n","        [ 2.91208791e-01,  6.92307692e-01,  9.89010989e-02,\n","          3.24607330e-01,  5.30120482e-01,  8.66025404e-01,\n","         -5.00000000e-01,  0.00000000e+00,  1.00000000e+00,\n","          1.00000000e+00,  6.12323400e-17]],\n","\n","       [[ 4.67032967e-01,  6.92307692e-01,  1.04395604e-01,\n","          4.92146597e-01,  4.57831325e-01,  1.00000000e+00,\n","          6.12323400e-17,  0.00000000e+00,  1.00000000e+00,\n","          8.66025404e-01,  5.00000000e-01],\n","        [ 2.91208791e-01,  6.92307692e-01,  9.89010989e-02,\n","          3.24607330e-01,  5.30120482e-01,  8.66025404e-01,\n","         -5.00000000e-01,  0.00000000e+00,  1.00000000e+00,\n","          1.00000000e+00,  6.12323400e-17],\n","        [ 2.36263736e-01,  3.46153846e-01,  8.79120879e-02,\n","          2.72251309e-01,  3.73493976e-01,  5.00000000e-01,\n","         -8.66025404e-01,  0.00000000e+00,  1.00000000e+00,\n","          8.66025404e-01, -5.00000000e-01]]])"]},"metadata":{},"execution_count":18}],"source":["x_train[:4, ]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1670389894660,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"zi5ZSeG6YJ0l","outputId":"84c74db1-de2a-4a0b-a2e8-2feebb9d4a70"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.46703297])"]},"metadata":{},"execution_count":19}],"source":["y_train[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1670389894661,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"Fo5BobQRX5zC","outputId":"37b0161b-7316-43c1-bd5a-4671d02d74e9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 3.90109890e-01,  4.61538462e-01,  1.20879121e-01,\n","         4.18848168e-01,  3.25301205e-01,  0.00000000e+00,\n","         1.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n","         1.22464680e-16, -1.00000000e+00],\n","       [ 3.46153846e-01,  5.38461538e-01,  1.04395604e-01,\n","         3.76963351e-01,  3.73493976e-01,  5.00000000e-01,\n","         8.66025404e-01,  0.00000000e+00,  1.00000000e+00,\n","         0.00000000e+00,  1.00000000e+00],\n","       [ 4.45054945e-01,  5.76923077e-01,  4.39560440e-02,\n","         4.71204188e-01,  3.49397590e-01,  8.66025404e-01,\n","         5.00000000e-01,  0.00000000e+00,  1.00000000e+00,\n","         5.00000000e-01,  8.66025404e-01]])"]},"metadata":{},"execution_count":20}],"source":["x_train[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1670389894661,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"35MYNVbiYRr9","outputId":"70fe3c49-45b9-41e7-e30e-ed7ae1857f77"},"outputs":[{"output_type":"stream","name":"stdout","text":["(3, 11) (1,)\n"]}],"source":["print(x_train[0].shape, y_train[0].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvW3Y4d8ZmO3"},"outputs":[],"source":["class TimeSeries(Dataset):\n","    def __init__(self,data, timesteps, n_features):\n","        # create sequences of length timesteps, including n_features for each item in the sequence \n","        data_timesteps = np.array([[j for j in data[i:i+timesteps+1]] for i in range(0,len(data)-timesteps)])[:,:,]\n","        x, y = data_timesteps[:,:-1, :], data_timesteps[:,-1:, 0]\n","\n","        self.x = torch.tensor(x,dtype=torch.float32)\n","        self.y = torch.tensor(y,dtype=torch.float32)\n","        self.len = len(x)\n","\n","    def __getitem__(self,idx):\n","        return self.x[idx],self.y[idx]\n","  \n","    def __len__(self):\n","        return self.len"]},{"cell_type":"markdown","metadata":{"id":"lopkaN1Yd2H3"},"source":["# GRU "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQuSptANd24z"},"outputs":[],"source":["class GRU(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n","        super(GRU, self).__init__()\n","\n","        self.layer_dim = layer_dim\n","        self.hidden_dim = hidden_dim\n","\n","        self.gru = nn.GRU(\n","            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n","        )\n","\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out, _ = self.gru(x)\n","        out = out[:, -1]\n","        out = self.fc(out)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"gpA7ZhalfM-k"},"source":["# CNN-LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1670389894661,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"EfwAlP69XFBq","outputId":"31e8f0dc-ed46-4cd7-e284-a6c7802e363a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 3.90109890e-01,  3.46153846e-01,  4.45054945e-01],\n","       [ 4.61538462e-01,  5.38461538e-01,  5.76923077e-01],\n","       [ 1.20879121e-01,  1.04395604e-01,  4.39560440e-02],\n","       [ 4.18848168e-01,  3.76963351e-01,  4.71204188e-01],\n","       [ 3.25301205e-01,  3.73493976e-01,  3.49397590e-01],\n","       [ 0.00000000e+00,  5.00000000e-01,  8.66025404e-01],\n","       [ 1.00000000e+00,  8.66025404e-01,  5.00000000e-01],\n","       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n","       [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00],\n","       [ 1.22464680e-16,  0.00000000e+00,  5.00000000e-01],\n","       [-1.00000000e+00,  1.00000000e+00,  8.66025404e-01]])"]},"metadata":{},"execution_count":24}],"source":["x_train[0].T"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":255,"status":"ok","timestamp":1670389894913,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"Vvd6GAP0dxpd","outputId":"a423d3ce-c277-46e7-f3c5-31e3c55ed6d9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 3.9011e-01,  3.4615e-01,  4.4505e-01],\n","        [ 4.6154e-01,  5.3846e-01,  5.7692e-01],\n","        [ 1.2088e-01,  1.0440e-01,  4.3956e-02],\n","        [ 4.1885e-01,  3.7696e-01,  4.7120e-01],\n","        [ 3.2530e-01,  3.7349e-01,  3.4940e-01],\n","        [ 0.0000e+00,  5.0000e-01,  8.6603e-01],\n","        [ 1.0000e+00,  8.6603e-01,  5.0000e-01],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n","        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],\n","        [ 1.2246e-16,  0.0000e+00,  5.0000e-01],\n","        [-1.0000e+00,  1.0000e+00,  8.6603e-01]], dtype=torch.float64)"]},"metadata":{},"execution_count":25}],"source":["(torch.transpose(torch.tensor(x_train), dim0=1,dim1=2))[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167,"status":"ok","timestamp":1670389895078,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"e-_XnqVwWvte","outputId":"a240020d-f091-4205-b7f3-623eaf39898f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.4128, -0.0472],\n","         [ 0.0374, -0.1242],\n","         [ 0.0945, -0.1356],\n","         ...,\n","         [ 0.3332,  0.1691],\n","         [-0.0808, -0.2248],\n","         [ 0.5766,  0.4601]],\n","\n","        [[-0.0472,  0.1541],\n","         [-0.1242,  0.0049],\n","         [-0.1356, -0.2768],\n","         ...,\n","         [ 0.1691,  0.2953],\n","         [-0.2248, -0.1074],\n","         [ 0.4601,  0.2472]],\n","\n","        [[ 0.1541,  0.2681],\n","         [ 0.0049,  0.1774],\n","         [-0.2768, -0.3899],\n","         ...,\n","         [ 0.2953,  0.3627],\n","         [-0.1074,  0.0348],\n","         [ 0.2472, -0.0650]],\n","\n","        ...,\n","\n","        [[-0.0755, -0.0840],\n","         [ 0.0447,  0.2479],\n","         [-0.2889, -0.4055],\n","         ...,\n","         [ 0.1212,  0.2284],\n","         [-0.1297,  0.2378],\n","         [ 0.3006,  0.1268]],\n","\n","        [[-0.0840, -0.1955],\n","         [ 0.2479,  0.4106],\n","         [-0.4055, -0.3959],\n","         ...,\n","         [ 0.2284,  0.3408],\n","         [ 0.2378,  0.5532],\n","         [ 0.1268, -0.0686]],\n","\n","        [[-0.1955, -0.3832],\n","         [ 0.4106,  0.4158],\n","         [-0.3959,  0.1755],\n","         ...,\n","         [ 0.3408,  0.5854],\n","         [ 0.5532,  0.3962],\n","         [-0.0686, -0.0415]]], dtype=torch.float64,\n","       grad_fn=<ConvolutionBackward0>)"]},"metadata":{},"execution_count":26}],"source":["c = nn.Conv1d(in_channels=11, out_channels=11, kernel_size=2, stride=1)\n","c = c.double()\n","input = torch.transpose(torch.tensor(x_train), dim0=1,dim1=2) \n","input = input.double() \n","output = c(input)\n","output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bk3OKNBFN7DF"},"outputs":[],"source":["class CNNLSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob, window, n_filters):\n","        super(CNNLSTM, self).__init__()\n","\n","        self.layer_dim = layer_dim\n","        self.hidden_dim = hidden_dim\n","\n","        self.conv1d = nn.Conv1d(in_channels=input_dim, out_channels=n_filters, kernel_size = window, stride = 1) # TODO: TUNE THIS \n","\n","        self.lstm = nn.LSTM(\n","            n_filters, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n","        )\n","\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        x = torch.transpose(x, dim0=1, dim1=2)\n","        out = self.conv1d(x)\n","        out = torch.transpose(out, dim0=1, dim1=2)\n","        out, _ = self.lstm(out)\n","        out = out[:, -1]\n","        out = self.fc(out)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"nULznrLefH55"},"source":["# Train "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGwfBKKXUNet"},"outputs":[],"source":["# TODO: use GPU "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AmKknyQbnvcD"},"outputs":[],"source":["N_FEATURES = train.shape[1]\n","INPUT_DIM = N_FEATURES\n","OUTPUT_DIM = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SH-Xpp8R4VS3"},"outputs":[],"source":["# GRU Hyperparameters\n","TIMESTEPS = 10\n","LEARNING_RATE = 0.001\n","N_EPOCHS = 100\n","BATCH_SIZE = 32\n","DROPOUT = 0 # TODO: CHANGE LATER\n","WEIGHT_DECAY = 0 # TODO: CHANGE LATER\n","HIDDEN_DIM = 128\n","LAYER_DIM = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"utUN3lXMb-VL"},"outputs":[],"source":["# Additional CNN-LSTM Hyperparameters \n","WINDOW = 3\n","N_FILTERS = N_FEATURES "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9b1mfWOiZvgD"},"outputs":[],"source":["train_ts = TimeSeries(train, TIMESTEPS, N_FEATURES)\n","test_ts = TimeSeries(test, TIMESTEPS, N_FEATURES)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-VPQCOgd0Nm"},"outputs":[],"source":["train_loader = DataLoader(train_ts, shuffle=True, batch_size=BATCH_SIZE)\n","test_loader = DataLoader(test_ts, shuffle=True, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Bh_zYdwcaYI"},"outputs":[],"source":[" model = CNNLSTM(INPUT_DIM, HIDDEN_DIM, LAYER_DIM, OUTPUT_DIM, DROPOUT, WINDOW, N_FILTERS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DutuzMXH5uBd"},"outputs":[],"source":["# model = GRU(INPUT_DIM, HIDDEN_DIM, LAYER_DIM, OUTPUT_DIM, DROPOUT) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"noo65YOqfKep"},"outputs":[],"source":["criterion = torch.nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjYItHni5qSI"},"outputs":[],"source":["def train_model(model, train_loader, test_loader, criterion, optimizer, \n","          num_epochs=N_EPOCHS, learning_rate=LEARNING_RATE, verbose=True):\n","\n","    for epoch in range(1, num_epochs + 1):\n","        batch_num = 1\n","        for inputs, targets in train_loader:  \n","\n","            # If using GPU \n","            if torch.cuda.is_available():\n","                model = model.cuda()\n","                inputs = inputs.cuda()\n","                targets = targets.cuda()\n","\n","            optimizer.zero_grad() \n","            output = model(inputs)\n","            loss = criterion(output, targets)\n","            loss.backward()\n","            optimizer.step() \n","\n","            if verbose and batch_num % 10 == 0:  # Print every 5 batches                              \n","                print(f'Epoch [{epoch}/{num_epochs}], Step [{batch_num}/{len(train_loader)}], '\n","                      f'Loss: {loss.item():.4f}')\n","            \n","            batch_num += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248157,"status":"ok","timestamp":1669693557820,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"Im7B8XBRA-Bf","outputId":"5e72233a-2716-46ca-c469-e5762866a5d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/100], Step [10/91], Loss: 0.0058\n","Epoch [1/100], Step [20/91], Loss: 0.0104\n","Epoch [1/100], Step [30/91], Loss: 0.0097\n","Epoch [1/100], Step [40/91], Loss: 0.0126\n","Epoch [1/100], Step [50/91], Loss: 0.0049\n","Epoch [1/100], Step [60/91], Loss: 0.0256\n","Epoch [1/100], Step [70/91], Loss: 0.0111\n","Epoch [1/100], Step [80/91], Loss: 0.0248\n","Epoch [1/100], Step [90/91], Loss: 0.0201\n","Epoch [2/100], Step [10/91], Loss: 0.0049\n","Epoch [2/100], Step [20/91], Loss: 0.0079\n","Epoch [2/100], Step [30/91], Loss: 0.0045\n","Epoch [2/100], Step [40/91], Loss: 0.0150\n","Epoch [2/100], Step [50/91], Loss: 0.0030\n","Epoch [2/100], Step [60/91], Loss: 0.0082\n","Epoch [2/100], Step [70/91], Loss: 0.0051\n","Epoch [2/100], Step [80/91], Loss: 0.0045\n","Epoch [2/100], Step [90/91], Loss: 0.0055\n","Epoch [3/100], Step [10/91], Loss: 0.0053\n","Epoch [3/100], Step [20/91], Loss: 0.0082\n","Epoch [3/100], Step [30/91], Loss: 0.0077\n","Epoch [3/100], Step [40/91], Loss: 0.0083\n","Epoch [3/100], Step [50/91], Loss: 0.0064\n","Epoch [3/100], Step [60/91], Loss: 0.0107\n","Epoch [3/100], Step [70/91], Loss: 0.0047\n","Epoch [3/100], Step [80/91], Loss: 0.0062\n","Epoch [3/100], Step [90/91], Loss: 0.0054\n","Epoch [4/100], Step [10/91], Loss: 0.0100\n","Epoch [4/100], Step [20/91], Loss: 0.0036\n","Epoch [4/100], Step [30/91], Loss: 0.0059\n","Epoch [4/100], Step [40/91], Loss: 0.0105\n","Epoch [4/100], Step [50/91], Loss: 0.0053\n","Epoch [4/100], Step [60/91], Loss: 0.0043\n","Epoch [4/100], Step [70/91], Loss: 0.0035\n","Epoch [4/100], Step [80/91], Loss: 0.0068\n","Epoch [4/100], Step [90/91], Loss: 0.0068\n","Epoch [5/100], Step [10/91], Loss: 0.0033\n","Epoch [5/100], Step [20/91], Loss: 0.0130\n","Epoch [5/100], Step [30/91], Loss: 0.0039\n","Epoch [5/100], Step [40/91], Loss: 0.0027\n","Epoch [5/100], Step [50/91], Loss: 0.0024\n","Epoch [5/100], Step [60/91], Loss: 0.0025\n","Epoch [5/100], Step [70/91], Loss: 0.0030\n","Epoch [5/100], Step [80/91], Loss: 0.0068\n","Epoch [5/100], Step [90/91], Loss: 0.0144\n","Epoch [6/100], Step [10/91], Loss: 0.0063\n","Epoch [6/100], Step [20/91], Loss: 0.0057\n","Epoch [6/100], Step [30/91], Loss: 0.0025\n","Epoch [6/100], Step [40/91], Loss: 0.0033\n","Epoch [6/100], Step [50/91], Loss: 0.0102\n","Epoch [6/100], Step [60/91], Loss: 0.0035\n","Epoch [6/100], Step [70/91], Loss: 0.0065\n","Epoch [6/100], Step [80/91], Loss: 0.0030\n","Epoch [6/100], Step [90/91], Loss: 0.0039\n","Epoch [7/100], Step [10/91], Loss: 0.0019\n","Epoch [7/100], Step [20/91], Loss: 0.0048\n","Epoch [7/100], Step [30/91], Loss: 0.0033\n","Epoch [7/100], Step [40/91], Loss: 0.0115\n","Epoch [7/100], Step [50/91], Loss: 0.0028\n","Epoch [7/100], Step [60/91], Loss: 0.0032\n","Epoch [7/100], Step [70/91], Loss: 0.0025\n","Epoch [7/100], Step [80/91], Loss: 0.0039\n","Epoch [7/100], Step [90/91], Loss: 0.0049\n","Epoch [8/100], Step [10/91], Loss: 0.0069\n","Epoch [8/100], Step [20/91], Loss: 0.0056\n","Epoch [8/100], Step [30/91], Loss: 0.0069\n","Epoch [8/100], Step [40/91], Loss: 0.0033\n","Epoch [8/100], Step [50/91], Loss: 0.0027\n","Epoch [8/100], Step [60/91], Loss: 0.0042\n","Epoch [8/100], Step [70/91], Loss: 0.0038\n","Epoch [8/100], Step [80/91], Loss: 0.0101\n","Epoch [8/100], Step [90/91], Loss: 0.0018\n","Epoch [9/100], Step [10/91], Loss: 0.0111\n","Epoch [9/100], Step [20/91], Loss: 0.0024\n","Epoch [9/100], Step [30/91], Loss: 0.0033\n","Epoch [9/100], Step [40/91], Loss: 0.0027\n","Epoch [9/100], Step [50/91], Loss: 0.0016\n","Epoch [9/100], Step [60/91], Loss: 0.0072\n","Epoch [9/100], Step [70/91], Loss: 0.0090\n","Epoch [9/100], Step [80/91], Loss: 0.0051\n","Epoch [9/100], Step [90/91], Loss: 0.0051\n","Epoch [10/100], Step [10/91], Loss: 0.0093\n","Epoch [10/100], Step [20/91], Loss: 0.0027\n","Epoch [10/100], Step [30/91], Loss: 0.0033\n","Epoch [10/100], Step [40/91], Loss: 0.0100\n","Epoch [10/100], Step [50/91], Loss: 0.0054\n","Epoch [10/100], Step [60/91], Loss: 0.0057\n","Epoch [10/100], Step [70/91], Loss: 0.0048\n","Epoch [10/100], Step [80/91], Loss: 0.0036\n","Epoch [10/100], Step [90/91], Loss: 0.0059\n","Epoch [11/100], Step [10/91], Loss: 0.0033\n","Epoch [11/100], Step [20/91], Loss: 0.0033\n","Epoch [11/100], Step [30/91], Loss: 0.0032\n","Epoch [11/100], Step [40/91], Loss: 0.0023\n","Epoch [11/100], Step [50/91], Loss: 0.0063\n","Epoch [11/100], Step [60/91], Loss: 0.0052\n","Epoch [11/100], Step [70/91], Loss: 0.0047\n","Epoch [11/100], Step [80/91], Loss: 0.0041\n","Epoch [11/100], Step [90/91], Loss: 0.0023\n","Epoch [12/100], Step [10/91], Loss: 0.0059\n","Epoch [12/100], Step [20/91], Loss: 0.0034\n","Epoch [12/100], Step [30/91], Loss: 0.0044\n","Epoch [12/100], Step [40/91], Loss: 0.0028\n","Epoch [12/100], Step [50/91], Loss: 0.0046\n","Epoch [12/100], Step [60/91], Loss: 0.0021\n","Epoch [12/100], Step [70/91], Loss: 0.0058\n","Epoch [12/100], Step [80/91], Loss: 0.0023\n","Epoch [12/100], Step [90/91], Loss: 0.0040\n","Epoch [13/100], Step [10/91], Loss: 0.0039\n","Epoch [13/100], Step [20/91], Loss: 0.0044\n","Epoch [13/100], Step [30/91], Loss: 0.0071\n","Epoch [13/100], Step [40/91], Loss: 0.0056\n","Epoch [13/100], Step [50/91], Loss: 0.0046\n","Epoch [13/100], Step [60/91], Loss: 0.0021\n","Epoch [13/100], Step [70/91], Loss: 0.0099\n","Epoch [13/100], Step [80/91], Loss: 0.0053\n","Epoch [13/100], Step [90/91], Loss: 0.0058\n","Epoch [14/100], Step [10/91], Loss: 0.0076\n","Epoch [14/100], Step [20/91], Loss: 0.0033\n","Epoch [14/100], Step [30/91], Loss: 0.0019\n","Epoch [14/100], Step [40/91], Loss: 0.0051\n","Epoch [14/100], Step [50/91], Loss: 0.0033\n","Epoch [14/100], Step [60/91], Loss: 0.0034\n","Epoch [14/100], Step [70/91], Loss: 0.0103\n","Epoch [14/100], Step [80/91], Loss: 0.0049\n","Epoch [14/100], Step [90/91], Loss: 0.0023\n","Epoch [15/100], Step [10/91], Loss: 0.0032\n","Epoch [15/100], Step [20/91], Loss: 0.0034\n","Epoch [15/100], Step [30/91], Loss: 0.0028\n","Epoch [15/100], Step [40/91], Loss: 0.0033\n","Epoch [15/100], Step [50/91], Loss: 0.0055\n","Epoch [15/100], Step [60/91], Loss: 0.0038\n","Epoch [15/100], Step [70/91], Loss: 0.0046\n","Epoch [15/100], Step [80/91], Loss: 0.0052\n","Epoch [15/100], Step [90/91], Loss: 0.0076\n","Epoch [16/100], Step [10/91], Loss: 0.0047\n","Epoch [16/100], Step [20/91], Loss: 0.0035\n","Epoch [16/100], Step [30/91], Loss: 0.0185\n","Epoch [16/100], Step [40/91], Loss: 0.0027\n","Epoch [16/100], Step [50/91], Loss: 0.0055\n","Epoch [16/100], Step [60/91], Loss: 0.0137\n","Epoch [16/100], Step [70/91], Loss: 0.0055\n","Epoch [16/100], Step [80/91], Loss: 0.0035\n","Epoch [16/100], Step [90/91], Loss: 0.0041\n","Epoch [17/100], Step [10/91], Loss: 0.0058\n","Epoch [17/100], Step [20/91], Loss: 0.0024\n","Epoch [17/100], Step [30/91], Loss: 0.0049\n","Epoch [17/100], Step [40/91], Loss: 0.0018\n","Epoch [17/100], Step [50/91], Loss: 0.0051\n","Epoch [17/100], Step [60/91], Loss: 0.0042\n","Epoch [17/100], Step [70/91], Loss: 0.0037\n","Epoch [17/100], Step [80/91], Loss: 0.0033\n","Epoch [17/100], Step [90/91], Loss: 0.0020\n","Epoch [18/100], Step [10/91], Loss: 0.0030\n","Epoch [18/100], Step [20/91], Loss: 0.0081\n","Epoch [18/100], Step [30/91], Loss: 0.0084\n","Epoch [18/100], Step [40/91], Loss: 0.0048\n","Epoch [18/100], Step [50/91], Loss: 0.0027\n","Epoch [18/100], Step [60/91], Loss: 0.0027\n","Epoch [18/100], Step [70/91], Loss: 0.0031\n","Epoch [18/100], Step [80/91], Loss: 0.0056\n","Epoch [18/100], Step [90/91], Loss: 0.0024\n","Epoch [19/100], Step [10/91], Loss: 0.0051\n","Epoch [19/100], Step [20/91], Loss: 0.0034\n","Epoch [19/100], Step [30/91], Loss: 0.0028\n","Epoch [19/100], Step [40/91], Loss: 0.0024\n","Epoch [19/100], Step [50/91], Loss: 0.0029\n","Epoch [19/100], Step [60/91], Loss: 0.0051\n","Epoch [19/100], Step [70/91], Loss: 0.0064\n","Epoch [19/100], Step [80/91], Loss: 0.0027\n","Epoch [19/100], Step [90/91], Loss: 0.0052\n","Epoch [20/100], Step [10/91], Loss: 0.0020\n","Epoch [20/100], Step [20/91], Loss: 0.0026\n","Epoch [20/100], Step [30/91], Loss: 0.0055\n","Epoch [20/100], Step [40/91], Loss: 0.0037\n","Epoch [20/100], Step [50/91], Loss: 0.0052\n","Epoch [20/100], Step [60/91], Loss: 0.0049\n","Epoch [20/100], Step [70/91], Loss: 0.0041\n","Epoch [20/100], Step [80/91], Loss: 0.0037\n","Epoch [20/100], Step [90/91], Loss: 0.0028\n","Epoch [21/100], Step [10/91], Loss: 0.0043\n","Epoch [21/100], Step [20/91], Loss: 0.0021\n","Epoch [21/100], Step [30/91], Loss: 0.0034\n","Epoch [21/100], Step [40/91], Loss: 0.0032\n","Epoch [21/100], Step [50/91], Loss: 0.0051\n","Epoch [21/100], Step [60/91], Loss: 0.0111\n","Epoch [21/100], Step [70/91], Loss: 0.0047\n","Epoch [21/100], Step [80/91], Loss: 0.0050\n","Epoch [21/100], Step [90/91], Loss: 0.0052\n","Epoch [22/100], Step [10/91], Loss: 0.0021\n","Epoch [22/100], Step [20/91], Loss: 0.0026\n","Epoch [22/100], Step [30/91], Loss: 0.0016\n","Epoch [22/100], Step [40/91], Loss: 0.0062\n","Epoch [22/100], Step [50/91], Loss: 0.0032\n","Epoch [22/100], Step [60/91], Loss: 0.0025\n","Epoch [22/100], Step [70/91], Loss: 0.0049\n","Epoch [22/100], Step [80/91], Loss: 0.0049\n","Epoch [22/100], Step [90/91], Loss: 0.0042\n","Epoch [23/100], Step [10/91], Loss: 0.0049\n","Epoch [23/100], Step [20/91], Loss: 0.0032\n","Epoch [23/100], Step [30/91], Loss: 0.0022\n","Epoch [23/100], Step [40/91], Loss: 0.0046\n","Epoch [23/100], Step [50/91], Loss: 0.0050\n","Epoch [23/100], Step [60/91], Loss: 0.0089\n","Epoch [23/100], Step [70/91], Loss: 0.0063\n","Epoch [23/100], Step [80/91], Loss: 0.0014\n","Epoch [23/100], Step [90/91], Loss: 0.0022\n","Epoch [24/100], Step [10/91], Loss: 0.0138\n","Epoch [24/100], Step [20/91], Loss: 0.0092\n","Epoch [24/100], Step [30/91], Loss: 0.0016\n","Epoch [24/100], Step [40/91], Loss: 0.0026\n","Epoch [24/100], Step [50/91], Loss: 0.0043\n","Epoch [24/100], Step [60/91], Loss: 0.0077\n","Epoch [24/100], Step [70/91], Loss: 0.0076\n","Epoch [24/100], Step [80/91], Loss: 0.0041\n","Epoch [24/100], Step [90/91], Loss: 0.0043\n","Epoch [25/100], Step [10/91], Loss: 0.0065\n","Epoch [25/100], Step [20/91], Loss: 0.0051\n","Epoch [25/100], Step [30/91], Loss: 0.0041\n","Epoch [25/100], Step [40/91], Loss: 0.0073\n","Epoch [25/100], Step [50/91], Loss: 0.0028\n","Epoch [25/100], Step [60/91], Loss: 0.0020\n","Epoch [25/100], Step [70/91], Loss: 0.0031\n","Epoch [25/100], Step [80/91], Loss: 0.0023\n","Epoch [25/100], Step [90/91], Loss: 0.0045\n","Epoch [26/100], Step [10/91], Loss: 0.0026\n","Epoch [26/100], Step [20/91], Loss: 0.0145\n","Epoch [26/100], Step [30/91], Loss: 0.0035\n","Epoch [26/100], Step [40/91], Loss: 0.0039\n","Epoch [26/100], Step [50/91], Loss: 0.0029\n","Epoch [26/100], Step [60/91], Loss: 0.0037\n","Epoch [26/100], Step [70/91], Loss: 0.0013\n","Epoch [26/100], Step [80/91], Loss: 0.0049\n","Epoch [26/100], Step [90/91], Loss: 0.0067\n","Epoch [27/100], Step [10/91], Loss: 0.0030\n","Epoch [27/100], Step [20/91], Loss: 0.0039\n","Epoch [27/100], Step [30/91], Loss: 0.0051\n","Epoch [27/100], Step [40/91], Loss: 0.0043\n","Epoch [27/100], Step [50/91], Loss: 0.0042\n","Epoch [27/100], Step [60/91], Loss: 0.0050\n","Epoch [27/100], Step [70/91], Loss: 0.0028\n","Epoch [27/100], Step [80/91], Loss: 0.0031\n","Epoch [27/100], Step [90/91], Loss: 0.0037\n","Epoch [28/100], Step [10/91], Loss: 0.0022\n","Epoch [28/100], Step [20/91], Loss: 0.0046\n","Epoch [28/100], Step [30/91], Loss: 0.0026\n","Epoch [28/100], Step [40/91], Loss: 0.0037\n","Epoch [28/100], Step [50/91], Loss: 0.0075\n","Epoch [28/100], Step [60/91], Loss: 0.0023\n","Epoch [28/100], Step [70/91], Loss: 0.0013\n","Epoch [28/100], Step [80/91], Loss: 0.0045\n","Epoch [28/100], Step [90/91], Loss: 0.0064\n","Epoch [29/100], Step [10/91], Loss: 0.0018\n","Epoch [29/100], Step [20/91], Loss: 0.0040\n","Epoch [29/100], Step [30/91], Loss: 0.0100\n","Epoch [29/100], Step [40/91], Loss: 0.0038\n","Epoch [29/100], Step [50/91], Loss: 0.0013\n","Epoch [29/100], Step [60/91], Loss: 0.0075\n","Epoch [29/100], Step [70/91], Loss: 0.0029\n","Epoch [29/100], Step [80/91], Loss: 0.0039\n","Epoch [29/100], Step [90/91], Loss: 0.0045\n","Epoch [30/100], Step [10/91], Loss: 0.0023\n","Epoch [30/100], Step [20/91], Loss: 0.0044\n","Epoch [30/100], Step [30/91], Loss: 0.0032\n","Epoch [30/100], Step [40/91], Loss: 0.0032\n","Epoch [30/100], Step [50/91], Loss: 0.0055\n","Epoch [30/100], Step [60/91], Loss: 0.0114\n","Epoch [30/100], Step [70/91], Loss: 0.0026\n","Epoch [30/100], Step [80/91], Loss: 0.0036\n","Epoch [30/100], Step [90/91], Loss: 0.0035\n","Epoch [31/100], Step [10/91], Loss: 0.0026\n","Epoch [31/100], Step [20/91], Loss: 0.0026\n","Epoch [31/100], Step [30/91], Loss: 0.0055\n","Epoch [31/100], Step [40/91], Loss: 0.0043\n","Epoch [31/100], Step [50/91], Loss: 0.0113\n","Epoch [31/100], Step [60/91], Loss: 0.0011\n","Epoch [31/100], Step [70/91], Loss: 0.0029\n","Epoch [31/100], Step [80/91], Loss: 0.0045\n","Epoch [31/100], Step [90/91], Loss: 0.0039\n","Epoch [32/100], Step [10/91], Loss: 0.0039\n","Epoch [32/100], Step [20/91], Loss: 0.0037\n","Epoch [32/100], Step [30/91], Loss: 0.0041\n","Epoch [32/100], Step [40/91], Loss: 0.0084\n","Epoch [32/100], Step [50/91], Loss: 0.0066\n","Epoch [32/100], Step [60/91], Loss: 0.0036\n","Epoch [32/100], Step [70/91], Loss: 0.0028\n","Epoch [32/100], Step [80/91], Loss: 0.0036\n","Epoch [32/100], Step [90/91], Loss: 0.0040\n","Epoch [33/100], Step [10/91], Loss: 0.0030\n","Epoch [33/100], Step [20/91], Loss: 0.0033\n","Epoch [33/100], Step [30/91], Loss: 0.0028\n","Epoch [33/100], Step [40/91], Loss: 0.0034\n","Epoch [33/100], Step [50/91], Loss: 0.0068\n","Epoch [33/100], Step [60/91], Loss: 0.0025\n","Epoch [33/100], Step [70/91], Loss: 0.0022\n","Epoch [33/100], Step [80/91], Loss: 0.0054\n","Epoch [33/100], Step [90/91], Loss: 0.0116\n","Epoch [34/100], Step [10/91], Loss: 0.0036\n","Epoch [34/100], Step [20/91], Loss: 0.0024\n","Epoch [34/100], Step [30/91], Loss: 0.0030\n","Epoch [34/100], Step [40/91], Loss: 0.0057\n","Epoch [34/100], Step [50/91], Loss: 0.0020\n","Epoch [34/100], Step [60/91], Loss: 0.0042\n","Epoch [34/100], Step [70/91], Loss: 0.0051\n","Epoch [34/100], Step [80/91], Loss: 0.0056\n","Epoch [34/100], Step [90/91], Loss: 0.0044\n","Epoch [35/100], Step [10/91], Loss: 0.0017\n","Epoch [35/100], Step [20/91], Loss: 0.0015\n","Epoch [35/100], Step [30/91], Loss: 0.0034\n","Epoch [35/100], Step [40/91], Loss: 0.0036\n","Epoch [35/100], Step [50/91], Loss: 0.0026\n","Epoch [35/100], Step [60/91], Loss: 0.0023\n","Epoch [35/100], Step [70/91], Loss: 0.0126\n","Epoch [35/100], Step [80/91], Loss: 0.0039\n","Epoch [35/100], Step [90/91], Loss: 0.0060\n","Epoch [36/100], Step [10/91], Loss: 0.0035\n","Epoch [36/100], Step [20/91], Loss: 0.0026\n","Epoch [36/100], Step [30/91], Loss: 0.0039\n","Epoch [36/100], Step [40/91], Loss: 0.0038\n","Epoch [36/100], Step [50/91], Loss: 0.0034\n","Epoch [36/100], Step [60/91], Loss: 0.0049\n","Epoch [36/100], Step [70/91], Loss: 0.0037\n","Epoch [36/100], Step [80/91], Loss: 0.0021\n","Epoch [36/100], Step [90/91], Loss: 0.0028\n","Epoch [37/100], Step [10/91], Loss: 0.0028\n","Epoch [37/100], Step [20/91], Loss: 0.0072\n","Epoch [37/100], Step [30/91], Loss: 0.0053\n","Epoch [37/100], Step [40/91], Loss: 0.0072\n","Epoch [37/100], Step [50/91], Loss: 0.0030\n","Epoch [37/100], Step [60/91], Loss: 0.0030\n","Epoch [37/100], Step [70/91], Loss: 0.0053\n","Epoch [37/100], Step [80/91], Loss: 0.0023\n","Epoch [37/100], Step [90/91], Loss: 0.0048\n","Epoch [38/100], Step [10/91], Loss: 0.0026\n","Epoch [38/100], Step [20/91], Loss: 0.0040\n","Epoch [38/100], Step [30/91], Loss: 0.0069\n","Epoch [38/100], Step [40/91], Loss: 0.0067\n","Epoch [38/100], Step [50/91], Loss: 0.0061\n","Epoch [38/100], Step [60/91], Loss: 0.0019\n","Epoch [38/100], Step [70/91], Loss: 0.0033\n","Epoch [38/100], Step [80/91], Loss: 0.0046\n","Epoch [38/100], Step [90/91], Loss: 0.0032\n","Epoch [39/100], Step [10/91], Loss: 0.0038\n","Epoch [39/100], Step [20/91], Loss: 0.0027\n","Epoch [39/100], Step [30/91], Loss: 0.0021\n","Epoch [39/100], Step [40/91], Loss: 0.0033\n","Epoch [39/100], Step [50/91], Loss: 0.0077\n","Epoch [39/100], Step [60/91], Loss: 0.0031\n","Epoch [39/100], Step [70/91], Loss: 0.0035\n","Epoch [39/100], Step [80/91], Loss: 0.0029\n","Epoch [39/100], Step [90/91], Loss: 0.0022\n","Epoch [40/100], Step [10/91], Loss: 0.0031\n","Epoch [40/100], Step [20/91], Loss: 0.0036\n","Epoch [40/100], Step [30/91], Loss: 0.0027\n","Epoch [40/100], Step [40/91], Loss: 0.0049\n","Epoch [40/100], Step [50/91], Loss: 0.0038\n","Epoch [40/100], Step [60/91], Loss: 0.0080\n","Epoch [40/100], Step [70/91], Loss: 0.0048\n","Epoch [40/100], Step [80/91], Loss: 0.0049\n","Epoch [40/100], Step [90/91], Loss: 0.0039\n","Epoch [41/100], Step [10/91], Loss: 0.0055\n","Epoch [41/100], Step [20/91], Loss: 0.0034\n","Epoch [41/100], Step [30/91], Loss: 0.0036\n","Epoch [41/100], Step [40/91], Loss: 0.0058\n","Epoch [41/100], Step [50/91], Loss: 0.0050\n","Epoch [41/100], Step [60/91], Loss: 0.0070\n","Epoch [41/100], Step [70/91], Loss: 0.0016\n","Epoch [41/100], Step [80/91], Loss: 0.0037\n","Epoch [41/100], Step [90/91], Loss: 0.0025\n","Epoch [42/100], Step [10/91], Loss: 0.0057\n","Epoch [42/100], Step [20/91], Loss: 0.0025\n","Epoch [42/100], Step [30/91], Loss: 0.0021\n","Epoch [42/100], Step [40/91], Loss: 0.0041\n","Epoch [42/100], Step [50/91], Loss: 0.0043\n","Epoch [42/100], Step [60/91], Loss: 0.0042\n","Epoch [42/100], Step [70/91], Loss: 0.0030\n","Epoch [42/100], Step [80/91], Loss: 0.0041\n","Epoch [42/100], Step [90/91], Loss: 0.0027\n","Epoch [43/100], Step [10/91], Loss: 0.0022\n","Epoch [43/100], Step [20/91], Loss: 0.0029\n","Epoch [43/100], Step [30/91], Loss: 0.0049\n","Epoch [43/100], Step [40/91], Loss: 0.0020\n","Epoch [43/100], Step [50/91], Loss: 0.0024\n","Epoch [43/100], Step [60/91], Loss: 0.0023\n","Epoch [43/100], Step [70/91], Loss: 0.0025\n","Epoch [43/100], Step [80/91], Loss: 0.0082\n","Epoch [43/100], Step [90/91], Loss: 0.0043\n","Epoch [44/100], Step [10/91], Loss: 0.0029\n","Epoch [44/100], Step [20/91], Loss: 0.0034\n","Epoch [44/100], Step [30/91], Loss: 0.0032\n","Epoch [44/100], Step [40/91], Loss: 0.0077\n","Epoch [44/100], Step [50/91], Loss: 0.0028\n","Epoch [44/100], Step [60/91], Loss: 0.0049\n","Epoch [44/100], Step [70/91], Loss: 0.0028\n","Epoch [44/100], Step [80/91], Loss: 0.0027\n","Epoch [44/100], Step [90/91], Loss: 0.0059\n","Epoch [45/100], Step [10/91], Loss: 0.0041\n","Epoch [45/100], Step [20/91], Loss: 0.0035\n","Epoch [45/100], Step [30/91], Loss: 0.0073\n","Epoch [45/100], Step [40/91], Loss: 0.0046\n","Epoch [45/100], Step [50/91], Loss: 0.0036\n","Epoch [45/100], Step [60/91], Loss: 0.0051\n","Epoch [45/100], Step [70/91], Loss: 0.0062\n","Epoch [45/100], Step [80/91], Loss: 0.0057\n","Epoch [45/100], Step [90/91], Loss: 0.0031\n","Epoch [46/100], Step [10/91], Loss: 0.0029\n","Epoch [46/100], Step [20/91], Loss: 0.0021\n","Epoch [46/100], Step [30/91], Loss: 0.0038\n","Epoch [46/100], Step [40/91], Loss: 0.0053\n","Epoch [46/100], Step [50/91], Loss: 0.0074\n","Epoch [46/100], Step [60/91], Loss: 0.0037\n","Epoch [46/100], Step [70/91], Loss: 0.0037\n","Epoch [46/100], Step [80/91], Loss: 0.0026\n","Epoch [46/100], Step [90/91], Loss: 0.0032\n","Epoch [47/100], Step [10/91], Loss: 0.0023\n","Epoch [47/100], Step [20/91], Loss: 0.0067\n","Epoch [47/100], Step [30/91], Loss: 0.0022\n","Epoch [47/100], Step [40/91], Loss: 0.0036\n","Epoch [47/100], Step [50/91], Loss: 0.0031\n","Epoch [47/100], Step [60/91], Loss: 0.0032\n","Epoch [47/100], Step [70/91], Loss: 0.0021\n","Epoch [47/100], Step [80/91], Loss: 0.0052\n","Epoch [47/100], Step [90/91], Loss: 0.0072\n","Epoch [48/100], Step [10/91], Loss: 0.0033\n","Epoch [48/100], Step [20/91], Loss: 0.0035\n","Epoch [48/100], Step [30/91], Loss: 0.0023\n","Epoch [48/100], Step [40/91], Loss: 0.0021\n","Epoch [48/100], Step [50/91], Loss: 0.0056\n","Epoch [48/100], Step [60/91], Loss: 0.0061\n","Epoch [48/100], Step [70/91], Loss: 0.0103\n","Epoch [48/100], Step [80/91], Loss: 0.0026\n","Epoch [48/100], Step [90/91], Loss: 0.0037\n","Epoch [49/100], Step [10/91], Loss: 0.0040\n","Epoch [49/100], Step [20/91], Loss: 0.0068\n","Epoch [49/100], Step [30/91], Loss: 0.0100\n","Epoch [49/100], Step [40/91], Loss: 0.0046\n","Epoch [49/100], Step [50/91], Loss: 0.0023\n","Epoch [49/100], Step [60/91], Loss: 0.0050\n","Epoch [49/100], Step [70/91], Loss: 0.0037\n","Epoch [49/100], Step [80/91], Loss: 0.0038\n","Epoch [49/100], Step [90/91], Loss: 0.0029\n","Epoch [50/100], Step [10/91], Loss: 0.0030\n","Epoch [50/100], Step [20/91], Loss: 0.0028\n","Epoch [50/100], Step [30/91], Loss: 0.0072\n","Epoch [50/100], Step [40/91], Loss: 0.0051\n","Epoch [50/100], Step [50/91], Loss: 0.0038\n","Epoch [50/100], Step [60/91], Loss: 0.0031\n","Epoch [50/100], Step [70/91], Loss: 0.0047\n","Epoch [50/100], Step [80/91], Loss: 0.0056\n","Epoch [50/100], Step [90/91], Loss: 0.0047\n","Epoch [51/100], Step [10/91], Loss: 0.0028\n","Epoch [51/100], Step [20/91], Loss: 0.0104\n","Epoch [51/100], Step [30/91], Loss: 0.0056\n","Epoch [51/100], Step [40/91], Loss: 0.0025\n","Epoch [51/100], Step [50/91], Loss: 0.0063\n","Epoch [51/100], Step [60/91], Loss: 0.0050\n","Epoch [51/100], Step [70/91], Loss: 0.0027\n","Epoch [51/100], Step [80/91], Loss: 0.0040\n","Epoch [51/100], Step [90/91], Loss: 0.0039\n","Epoch [52/100], Step [10/91], Loss: 0.0031\n","Epoch [52/100], Step [20/91], Loss: 0.0036\n","Epoch [52/100], Step [30/91], Loss: 0.0026\n","Epoch [52/100], Step [40/91], Loss: 0.0039\n","Epoch [52/100], Step [50/91], Loss: 0.0034\n","Epoch [52/100], Step [60/91], Loss: 0.0020\n","Epoch [52/100], Step [70/91], Loss: 0.0028\n","Epoch [52/100], Step [80/91], Loss: 0.0017\n","Epoch [52/100], Step [90/91], Loss: 0.0048\n","Epoch [53/100], Step [10/91], Loss: 0.0083\n","Epoch [53/100], Step [20/91], Loss: 0.0020\n","Epoch [53/100], Step [30/91], Loss: 0.0049\n","Epoch [53/100], Step [40/91], Loss: 0.0019\n","Epoch [53/100], Step [50/91], Loss: 0.0039\n","Epoch [53/100], Step [60/91], Loss: 0.0057\n","Epoch [53/100], Step [70/91], Loss: 0.0024\n","Epoch [53/100], Step [80/91], Loss: 0.0052\n","Epoch [53/100], Step [90/91], Loss: 0.0019\n","Epoch [54/100], Step [10/91], Loss: 0.0044\n","Epoch [54/100], Step [20/91], Loss: 0.0065\n","Epoch [54/100], Step [30/91], Loss: 0.0038\n","Epoch [54/100], Step [40/91], Loss: 0.0014\n","Epoch [54/100], Step [50/91], Loss: 0.0025\n","Epoch [54/100], Step [60/91], Loss: 0.0030\n","Epoch [54/100], Step [70/91], Loss: 0.0103\n","Epoch [54/100], Step [80/91], Loss: 0.0062\n","Epoch [54/100], Step [90/91], Loss: 0.0035\n","Epoch [55/100], Step [10/91], Loss: 0.0034\n","Epoch [55/100], Step [20/91], Loss: 0.0028\n","Epoch [55/100], Step [30/91], Loss: 0.0042\n","Epoch [55/100], Step [40/91], Loss: 0.0034\n","Epoch [55/100], Step [50/91], Loss: 0.0031\n","Epoch [55/100], Step [60/91], Loss: 0.0038\n","Epoch [55/100], Step [70/91], Loss: 0.0019\n","Epoch [55/100], Step [80/91], Loss: 0.0034\n","Epoch [55/100], Step [90/91], Loss: 0.0046\n","Epoch [56/100], Step [10/91], Loss: 0.0033\n","Epoch [56/100], Step [20/91], Loss: 0.0019\n","Epoch [56/100], Step [30/91], Loss: 0.0070\n","Epoch [56/100], Step [40/91], Loss: 0.0054\n","Epoch [56/100], Step [50/91], Loss: 0.0022\n","Epoch [56/100], Step [60/91], Loss: 0.0030\n","Epoch [56/100], Step [70/91], Loss: 0.0032\n","Epoch [56/100], Step [80/91], Loss: 0.0037\n","Epoch [56/100], Step [90/91], Loss: 0.0011\n","Epoch [57/100], Step [10/91], Loss: 0.0017\n","Epoch [57/100], Step [20/91], Loss: 0.0033\n","Epoch [57/100], Step [30/91], Loss: 0.0023\n","Epoch [57/100], Step [40/91], Loss: 0.0038\n","Epoch [57/100], Step [50/91], Loss: 0.0125\n","Epoch [57/100], Step [60/91], Loss: 0.0025\n","Epoch [57/100], Step [70/91], Loss: 0.0026\n","Epoch [57/100], Step [80/91], Loss: 0.0037\n","Epoch [57/100], Step [90/91], Loss: 0.0037\n","Epoch [58/100], Step [10/91], Loss: 0.0020\n","Epoch [58/100], Step [20/91], Loss: 0.0045\n","Epoch [58/100], Step [30/91], Loss: 0.0033\n","Epoch [58/100], Step [40/91], Loss: 0.0030\n","Epoch [58/100], Step [50/91], Loss: 0.0032\n","Epoch [58/100], Step [60/91], Loss: 0.0032\n","Epoch [58/100], Step [70/91], Loss: 0.0078\n","Epoch [58/100], Step [80/91], Loss: 0.0042\n","Epoch [58/100], Step [90/91], Loss: 0.0031\n","Epoch [59/100], Step [10/91], Loss: 0.0024\n","Epoch [59/100], Step [20/91], Loss: 0.0040\n","Epoch [59/100], Step [30/91], Loss: 0.0055\n","Epoch [59/100], Step [40/91], Loss: 0.0047\n","Epoch [59/100], Step [50/91], Loss: 0.0031\n","Epoch [59/100], Step [60/91], Loss: 0.0030\n","Epoch [59/100], Step [70/91], Loss: 0.0040\n","Epoch [59/100], Step [80/91], Loss: 0.0035\n","Epoch [59/100], Step [90/91], Loss: 0.0033\n","Epoch [60/100], Step [10/91], Loss: 0.0015\n","Epoch [60/100], Step [20/91], Loss: 0.0019\n","Epoch [60/100], Step [30/91], Loss: 0.0046\n","Epoch [60/100], Step [40/91], Loss: 0.0038\n","Epoch [60/100], Step [50/91], Loss: 0.0045\n","Epoch [60/100], Step [60/91], Loss: 0.0022\n","Epoch [60/100], Step [70/91], Loss: 0.0034\n","Epoch [60/100], Step [80/91], Loss: 0.0094\n","Epoch [60/100], Step [90/91], Loss: 0.0051\n","Epoch [61/100], Step [10/91], Loss: 0.0017\n","Epoch [61/100], Step [20/91], Loss: 0.0018\n","Epoch [61/100], Step [30/91], Loss: 0.0036\n","Epoch [61/100], Step [40/91], Loss: 0.0026\n","Epoch [61/100], Step [50/91], Loss: 0.0037\n","Epoch [61/100], Step [60/91], Loss: 0.0017\n","Epoch [61/100], Step [70/91], Loss: 0.0031\n","Epoch [61/100], Step [80/91], Loss: 0.0034\n","Epoch [61/100], Step [90/91], Loss: 0.0070\n","Epoch [62/100], Step [10/91], Loss: 0.0016\n","Epoch [62/100], Step [20/91], Loss: 0.0019\n","Epoch [62/100], Step [30/91], Loss: 0.0030\n","Epoch [62/100], Step [40/91], Loss: 0.0028\n","Epoch [62/100], Step [50/91], Loss: 0.0021\n","Epoch [62/100], Step [60/91], Loss: 0.0040\n","Epoch [62/100], Step [70/91], Loss: 0.0033\n","Epoch [62/100], Step [80/91], Loss: 0.0028\n","Epoch [62/100], Step [90/91], Loss: 0.0023\n","Epoch [63/100], Step [10/91], Loss: 0.0018\n","Epoch [63/100], Step [20/91], Loss: 0.0017\n","Epoch [63/100], Step [30/91], Loss: 0.0020\n","Epoch [63/100], Step [40/91], Loss: 0.0024\n","Epoch [63/100], Step [50/91], Loss: 0.0022\n","Epoch [63/100], Step [60/91], Loss: 0.0065\n","Epoch [63/100], Step [70/91], Loss: 0.0034\n","Epoch [63/100], Step [80/91], Loss: 0.0087\n","Epoch [63/100], Step [90/91], Loss: 0.0056\n","Epoch [64/100], Step [10/91], Loss: 0.0015\n","Epoch [64/100], Step [20/91], Loss: 0.0021\n","Epoch [64/100], Step [30/91], Loss: 0.0032\n","Epoch [64/100], Step [40/91], Loss: 0.0025\n","Epoch [64/100], Step [50/91], Loss: 0.0030\n","Epoch [64/100], Step [60/91], Loss: 0.0050\n","Epoch [64/100], Step [70/91], Loss: 0.0032\n","Epoch [64/100], Step [80/91], Loss: 0.0028\n","Epoch [64/100], Step [90/91], Loss: 0.0041\n","Epoch [65/100], Step [10/91], Loss: 0.0027\n","Epoch [65/100], Step [20/91], Loss: 0.0029\n","Epoch [65/100], Step [30/91], Loss: 0.0027\n","Epoch [65/100], Step [40/91], Loss: 0.0049\n","Epoch [65/100], Step [50/91], Loss: 0.0056\n","Epoch [65/100], Step [60/91], Loss: 0.0020\n","Epoch [65/100], Step [70/91], Loss: 0.0046\n","Epoch [65/100], Step [80/91], Loss: 0.0062\n","Epoch [65/100], Step [90/91], Loss: 0.0022\n","Epoch [66/100], Step [10/91], Loss: 0.0029\n","Epoch [66/100], Step [20/91], Loss: 0.0018\n","Epoch [66/100], Step [30/91], Loss: 0.0083\n","Epoch [66/100], Step [40/91], Loss: 0.0021\n","Epoch [66/100], Step [50/91], Loss: 0.0063\n","Epoch [66/100], Step [60/91], Loss: 0.0030\n","Epoch [66/100], Step [70/91], Loss: 0.0019\n","Epoch [66/100], Step [80/91], Loss: 0.0029\n","Epoch [66/100], Step [90/91], Loss: 0.0036\n","Epoch [67/100], Step [10/91], Loss: 0.0022\n","Epoch [67/100], Step [20/91], Loss: 0.0054\n","Epoch [67/100], Step [30/91], Loss: 0.0026\n","Epoch [67/100], Step [40/91], Loss: 0.0031\n","Epoch [67/100], Step [50/91], Loss: 0.0025\n","Epoch [67/100], Step [60/91], Loss: 0.0018\n","Epoch [67/100], Step [70/91], Loss: 0.0013\n","Epoch [67/100], Step [80/91], Loss: 0.0043\n","Epoch [67/100], Step [90/91], Loss: 0.0018\n","Epoch [68/100], Step [10/91], Loss: 0.0015\n","Epoch [68/100], Step [20/91], Loss: 0.0021\n","Epoch [68/100], Step [30/91], Loss: 0.0045\n","Epoch [68/100], Step [40/91], Loss: 0.0026\n","Epoch [68/100], Step [50/91], Loss: 0.0019\n","Epoch [68/100], Step [60/91], Loss: 0.0027\n","Epoch [68/100], Step [70/91], Loss: 0.0022\n","Epoch [68/100], Step [80/91], Loss: 0.0030\n","Epoch [68/100], Step [90/91], Loss: 0.0014\n","Epoch [69/100], Step [10/91], Loss: 0.0021\n","Epoch [69/100], Step [20/91], Loss: 0.0033\n","Epoch [69/100], Step [30/91], Loss: 0.0013\n","Epoch [69/100], Step [40/91], Loss: 0.0018\n","Epoch [69/100], Step [50/91], Loss: 0.0018\n","Epoch [69/100], Step [60/91], Loss: 0.0020\n","Epoch [69/100], Step [70/91], Loss: 0.0021\n","Epoch [69/100], Step [80/91], Loss: 0.0028\n","Epoch [69/100], Step [90/91], Loss: 0.0026\n","Epoch [70/100], Step [10/91], Loss: 0.0044\n","Epoch [70/100], Step [20/91], Loss: 0.0020\n","Epoch [70/100], Step [30/91], Loss: 0.0032\n","Epoch [70/100], Step [40/91], Loss: 0.0031\n","Epoch [70/100], Step [50/91], Loss: 0.0020\n","Epoch [70/100], Step [60/91], Loss: 0.0017\n","Epoch [70/100], Step [70/91], Loss: 0.0028\n","Epoch [70/100], Step [80/91], Loss: 0.0033\n","Epoch [70/100], Step [90/91], Loss: 0.0056\n","Epoch [71/100], Step [10/91], Loss: 0.0012\n","Epoch [71/100], Step [20/91], Loss: 0.0039\n","Epoch [71/100], Step [30/91], Loss: 0.0034\n","Epoch [71/100], Step [40/91], Loss: 0.0019\n","Epoch [71/100], Step [50/91], Loss: 0.0022\n","Epoch [71/100], Step [60/91], Loss: 0.0017\n","Epoch [71/100], Step [70/91], Loss: 0.0023\n","Epoch [71/100], Step [80/91], Loss: 0.0028\n","Epoch [71/100], Step [90/91], Loss: 0.0013\n","Epoch [72/100], Step [10/91], Loss: 0.0023\n","Epoch [72/100], Step [20/91], Loss: 0.0010\n","Epoch [72/100], Step [30/91], Loss: 0.0020\n","Epoch [72/100], Step [40/91], Loss: 0.0020\n","Epoch [72/100], Step [50/91], Loss: 0.0023\n","Epoch [72/100], Step [60/91], Loss: 0.0026\n","Epoch [72/100], Step [70/91], Loss: 0.0029\n","Epoch [72/100], Step [80/91], Loss: 0.0018\n","Epoch [72/100], Step [90/91], Loss: 0.0016\n","Epoch [73/100], Step [10/91], Loss: 0.0021\n","Epoch [73/100], Step [20/91], Loss: 0.0019\n","Epoch [73/100], Step [30/91], Loss: 0.0013\n","Epoch [73/100], Step [40/91], Loss: 0.0012\n","Epoch [73/100], Step [50/91], Loss: 0.0027\n","Epoch [73/100], Step [60/91], Loss: 0.0016\n","Epoch [73/100], Step [70/91], Loss: 0.0017\n","Epoch [73/100], Step [80/91], Loss: 0.0030\n","Epoch [73/100], Step [90/91], Loss: 0.0019\n","Epoch [74/100], Step [10/91], Loss: 0.0018\n","Epoch [74/100], Step [20/91], Loss: 0.0024\n","Epoch [74/100], Step [30/91], Loss: 0.0017\n","Epoch [74/100], Step [40/91], Loss: 0.0042\n","Epoch [74/100], Step [50/91], Loss: 0.0026\n","Epoch [74/100], Step [60/91], Loss: 0.0027\n","Epoch [74/100], Step [70/91], Loss: 0.0028\n","Epoch [74/100], Step [80/91], Loss: 0.0023\n","Epoch [74/100], Step [90/91], Loss: 0.0020\n","Epoch [75/100], Step [10/91], Loss: 0.0024\n","Epoch [75/100], Step [20/91], Loss: 0.0036\n","Epoch [75/100], Step [30/91], Loss: 0.0018\n","Epoch [75/100], Step [40/91], Loss: 0.0019\n","Epoch [75/100], Step [50/91], Loss: 0.0013\n","Epoch [75/100], Step [60/91], Loss: 0.0023\n","Epoch [75/100], Step [70/91], Loss: 0.0020\n","Epoch [75/100], Step [80/91], Loss: 0.0020\n","Epoch [75/100], Step [90/91], Loss: 0.0017\n","Epoch [76/100], Step [10/91], Loss: 0.0013\n","Epoch [76/100], Step [20/91], Loss: 0.0013\n","Epoch [76/100], Step [30/91], Loss: 0.0033\n","Epoch [76/100], Step [40/91], Loss: 0.0012\n","Epoch [76/100], Step [50/91], Loss: 0.0013\n","Epoch [76/100], Step [60/91], Loss: 0.0013\n","Epoch [76/100], Step [70/91], Loss: 0.0016\n","Epoch [76/100], Step [80/91], Loss: 0.0021\n","Epoch [76/100], Step [90/91], Loss: 0.0022\n","Epoch [77/100], Step [10/91], Loss: 0.0023\n","Epoch [77/100], Step [20/91], Loss: 0.0014\n","Epoch [77/100], Step [30/91], Loss: 0.0012\n","Epoch [77/100], Step [40/91], Loss: 0.0011\n","Epoch [77/100], Step [50/91], Loss: 0.0016\n","Epoch [77/100], Step [60/91], Loss: 0.0017\n","Epoch [77/100], Step [70/91], Loss: 0.0010\n","Epoch [77/100], Step [80/91], Loss: 0.0031\n","Epoch [77/100], Step [90/91], Loss: 0.0014\n","Epoch [78/100], Step [10/91], Loss: 0.0014\n","Epoch [78/100], Step [20/91], Loss: 0.0021\n","Epoch [78/100], Step [30/91], Loss: 0.0014\n","Epoch [78/100], Step [40/91], Loss: 0.0020\n","Epoch [78/100], Step [50/91], Loss: 0.0018\n","Epoch [78/100], Step [60/91], Loss: 0.0013\n","Epoch [78/100], Step [70/91], Loss: 0.0023\n","Epoch [78/100], Step [80/91], Loss: 0.0010\n","Epoch [78/100], Step [90/91], Loss: 0.0020\n","Epoch [79/100], Step [10/91], Loss: 0.0022\n","Epoch [79/100], Step [20/91], Loss: 0.0012\n","Epoch [79/100], Step [30/91], Loss: 0.0018\n","Epoch [79/100], Step [40/91], Loss: 0.0010\n","Epoch [79/100], Step [50/91], Loss: 0.0022\n","Epoch [79/100], Step [60/91], Loss: 0.0017\n","Epoch [79/100], Step [70/91], Loss: 0.0022\n","Epoch [79/100], Step [80/91], Loss: 0.0025\n","Epoch [79/100], Step [90/91], Loss: 0.0016\n","Epoch [80/100], Step [10/91], Loss: 0.0010\n","Epoch [80/100], Step [20/91], Loss: 0.0012\n","Epoch [80/100], Step [30/91], Loss: 0.0018\n","Epoch [80/100], Step [40/91], Loss: 0.0011\n","Epoch [80/100], Step [50/91], Loss: 0.0010\n","Epoch [80/100], Step [60/91], Loss: 0.0018\n","Epoch [80/100], Step [70/91], Loss: 0.0020\n","Epoch [80/100], Step [80/91], Loss: 0.0019\n","Epoch [80/100], Step [90/91], Loss: 0.0005\n","Epoch [81/100], Step [10/91], Loss: 0.0016\n","Epoch [81/100], Step [20/91], Loss: 0.0012\n","Epoch [81/100], Step [30/91], Loss: 0.0011\n","Epoch [81/100], Step [40/91], Loss: 0.0022\n","Epoch [81/100], Step [50/91], Loss: 0.0034\n","Epoch [81/100], Step [60/91], Loss: 0.0011\n","Epoch [81/100], Step [70/91], Loss: 0.0015\n","Epoch [81/100], Step [80/91], Loss: 0.0012\n","Epoch [81/100], Step [90/91], Loss: 0.0025\n","Epoch [82/100], Step [10/91], Loss: 0.0014\n","Epoch [82/100], Step [20/91], Loss: 0.0011\n","Epoch [82/100], Step [30/91], Loss: 0.0017\n","Epoch [82/100], Step [40/91], Loss: 0.0012\n","Epoch [82/100], Step [50/91], Loss: 0.0015\n","Epoch [82/100], Step [60/91], Loss: 0.0014\n","Epoch [82/100], Step [70/91], Loss: 0.0022\n","Epoch [82/100], Step [80/91], Loss: 0.0014\n","Epoch [82/100], Step [90/91], Loss: 0.0011\n","Epoch [83/100], Step [10/91], Loss: 0.0012\n","Epoch [83/100], Step [20/91], Loss: 0.0011\n","Epoch [83/100], Step [30/91], Loss: 0.0014\n","Epoch [83/100], Step [40/91], Loss: 0.0014\n","Epoch [83/100], Step [50/91], Loss: 0.0017\n","Epoch [83/100], Step [60/91], Loss: 0.0016\n","Epoch [83/100], Step [70/91], Loss: 0.0017\n","Epoch [83/100], Step [80/91], Loss: 0.0019\n","Epoch [83/100], Step [90/91], Loss: 0.0018\n","Epoch [84/100], Step [10/91], Loss: 0.0015\n","Epoch [84/100], Step [20/91], Loss: 0.0008\n","Epoch [84/100], Step [30/91], Loss: 0.0015\n","Epoch [84/100], Step [40/91], Loss: 0.0012\n","Epoch [84/100], Step [50/91], Loss: 0.0009\n","Epoch [84/100], Step [60/91], Loss: 0.0012\n","Epoch [84/100], Step [70/91], Loss: 0.0008\n","Epoch [84/100], Step [80/91], Loss: 0.0018\n","Epoch [84/100], Step [90/91], Loss: 0.0012\n","Epoch [85/100], Step [10/91], Loss: 0.0008\n","Epoch [85/100], Step [20/91], Loss: 0.0010\n","Epoch [85/100], Step [30/91], Loss: 0.0009\n","Epoch [85/100], Step [40/91], Loss: 0.0017\n","Epoch [85/100], Step [50/91], Loss: 0.0013\n","Epoch [85/100], Step [60/91], Loss: 0.0013\n","Epoch [85/100], Step [70/91], Loss: 0.0008\n","Epoch [85/100], Step [80/91], Loss: 0.0011\n","Epoch [85/100], Step [90/91], Loss: 0.0015\n","Epoch [86/100], Step [10/91], Loss: 0.0009\n","Epoch [86/100], Step [20/91], Loss: 0.0007\n","Epoch [86/100], Step [30/91], Loss: 0.0008\n","Epoch [86/100], Step [40/91], Loss: 0.0009\n","Epoch [86/100], Step [50/91], Loss: 0.0013\n","Epoch [86/100], Step [60/91], Loss: 0.0013\n","Epoch [86/100], Step [70/91], Loss: 0.0011\n","Epoch [86/100], Step [80/91], Loss: 0.0013\n","Epoch [86/100], Step [90/91], Loss: 0.0012\n","Epoch [87/100], Step [10/91], Loss: 0.0014\n","Epoch [87/100], Step [20/91], Loss: 0.0010\n","Epoch [87/100], Step [30/91], Loss: 0.0012\n","Epoch [87/100], Step [40/91], Loss: 0.0008\n","Epoch [87/100], Step [50/91], Loss: 0.0008\n","Epoch [87/100], Step [60/91], Loss: 0.0011\n","Epoch [87/100], Step [70/91], Loss: 0.0010\n","Epoch [87/100], Step [80/91], Loss: 0.0019\n","Epoch [87/100], Step [90/91], Loss: 0.0008\n","Epoch [88/100], Step [10/91], Loss: 0.0010\n","Epoch [88/100], Step [20/91], Loss: 0.0008\n","Epoch [88/100], Step [30/91], Loss: 0.0012\n","Epoch [88/100], Step [40/91], Loss: 0.0008\n","Epoch [88/100], Step [50/91], Loss: 0.0017\n","Epoch [88/100], Step [60/91], Loss: 0.0008\n","Epoch [88/100], Step [70/91], Loss: 0.0019\n","Epoch [88/100], Step [80/91], Loss: 0.0009\n","Epoch [88/100], Step [90/91], Loss: 0.0011\n","Epoch [89/100], Step [10/91], Loss: 0.0011\n","Epoch [89/100], Step [20/91], Loss: 0.0010\n","Epoch [89/100], Step [30/91], Loss: 0.0009\n","Epoch [89/100], Step [40/91], Loss: 0.0007\n","Epoch [89/100], Step [50/91], Loss: 0.0011\n","Epoch [89/100], Step [60/91], Loss: 0.0012\n","Epoch [89/100], Step [70/91], Loss: 0.0014\n","Epoch [89/100], Step [80/91], Loss: 0.0008\n","Epoch [89/100], Step [90/91], Loss: 0.0005\n","Epoch [90/100], Step [10/91], Loss: 0.0007\n","Epoch [90/100], Step [20/91], Loss: 0.0012\n","Epoch [90/100], Step [30/91], Loss: 0.0011\n","Epoch [90/100], Step [40/91], Loss: 0.0007\n","Epoch [90/100], Step [50/91], Loss: 0.0009\n","Epoch [90/100], Step [60/91], Loss: 0.0011\n","Epoch [90/100], Step [70/91], Loss: 0.0017\n","Epoch [90/100], Step [80/91], Loss: 0.0015\n","Epoch [90/100], Step [90/91], Loss: 0.0008\n","Epoch [91/100], Step [10/91], Loss: 0.0005\n","Epoch [91/100], Step [20/91], Loss: 0.0013\n","Epoch [91/100], Step [30/91], Loss: 0.0007\n","Epoch [91/100], Step [40/91], Loss: 0.0009\n","Epoch [91/100], Step [50/91], Loss: 0.0008\n","Epoch [91/100], Step [60/91], Loss: 0.0010\n","Epoch [91/100], Step [70/91], Loss: 0.0011\n","Epoch [91/100], Step [80/91], Loss: 0.0009\n","Epoch [91/100], Step [90/91], Loss: 0.0011\n","Epoch [92/100], Step [10/91], Loss: 0.0006\n","Epoch [92/100], Step [20/91], Loss: 0.0009\n","Epoch [92/100], Step [30/91], Loss: 0.0005\n","Epoch [92/100], Step [40/91], Loss: 0.0009\n","Epoch [92/100], Step [50/91], Loss: 0.0006\n","Epoch [92/100], Step [60/91], Loss: 0.0006\n","Epoch [92/100], Step [70/91], Loss: 0.0009\n","Epoch [92/100], Step [80/91], Loss: 0.0009\n","Epoch [92/100], Step [90/91], Loss: 0.0010\n","Epoch [93/100], Step [10/91], Loss: 0.0009\n","Epoch [93/100], Step [20/91], Loss: 0.0007\n","Epoch [93/100], Step [30/91], Loss: 0.0008\n","Epoch [93/100], Step [40/91], Loss: 0.0006\n","Epoch [93/100], Step [50/91], Loss: 0.0011\n","Epoch [93/100], Step [60/91], Loss: 0.0011\n","Epoch [93/100], Step [70/91], Loss: 0.0008\n","Epoch [93/100], Step [80/91], Loss: 0.0012\n","Epoch [93/100], Step [90/91], Loss: 0.0006\n","Epoch [94/100], Step [10/91], Loss: 0.0009\n","Epoch [94/100], Step [20/91], Loss: 0.0008\n","Epoch [94/100], Step [30/91], Loss: 0.0005\n","Epoch [94/100], Step [40/91], Loss: 0.0008\n","Epoch [94/100], Step [50/91], Loss: 0.0008\n","Epoch [94/100], Step [60/91], Loss: 0.0009\n","Epoch [94/100], Step [70/91], Loss: 0.0007\n","Epoch [94/100], Step [80/91], Loss: 0.0006\n","Epoch [94/100], Step [90/91], Loss: 0.0013\n","Epoch [95/100], Step [10/91], Loss: 0.0009\n","Epoch [95/100], Step [20/91], Loss: 0.0008\n","Epoch [95/100], Step [30/91], Loss: 0.0008\n","Epoch [95/100], Step [40/91], Loss: 0.0006\n","Epoch [95/100], Step [50/91], Loss: 0.0008\n","Epoch [95/100], Step [60/91], Loss: 0.0006\n","Epoch [95/100], Step [70/91], Loss: 0.0008\n","Epoch [95/100], Step [80/91], Loss: 0.0007\n","Epoch [95/100], Step [90/91], Loss: 0.0009\n","Epoch [96/100], Step [10/91], Loss: 0.0005\n","Epoch [96/100], Step [20/91], Loss: 0.0004\n","Epoch [96/100], Step [30/91], Loss: 0.0006\n","Epoch [96/100], Step [40/91], Loss: 0.0008\n","Epoch [96/100], Step [50/91], Loss: 0.0011\n","Epoch [96/100], Step [60/91], Loss: 0.0008\n","Epoch [96/100], Step [70/91], Loss: 0.0006\n","Epoch [96/100], Step [80/91], Loss: 0.0010\n","Epoch [96/100], Step [90/91], Loss: 0.0007\n","Epoch [97/100], Step [10/91], Loss: 0.0007\n","Epoch [97/100], Step [20/91], Loss: 0.0008\n","Epoch [97/100], Step [30/91], Loss: 0.0008\n","Epoch [97/100], Step [40/91], Loss: 0.0012\n","Epoch [97/100], Step [50/91], Loss: 0.0006\n","Epoch [97/100], Step [60/91], Loss: 0.0004\n","Epoch [97/100], Step [70/91], Loss: 0.0008\n","Epoch [97/100], Step [80/91], Loss: 0.0007\n","Epoch [97/100], Step [90/91], Loss: 0.0010\n","Epoch [98/100], Step [10/91], Loss: 0.0008\n","Epoch [98/100], Step [20/91], Loss: 0.0008\n","Epoch [98/100], Step [30/91], Loss: 0.0008\n","Epoch [98/100], Step [40/91], Loss: 0.0006\n","Epoch [98/100], Step [50/91], Loss: 0.0010\n","Epoch [98/100], Step [60/91], Loss: 0.0006\n","Epoch [98/100], Step [70/91], Loss: 0.0008\n","Epoch [98/100], Step [80/91], Loss: 0.0005\n","Epoch [98/100], Step [90/91], Loss: 0.0006\n","Epoch [99/100], Step [10/91], Loss: 0.0008\n","Epoch [99/100], Step [20/91], Loss: 0.0006\n","Epoch [99/100], Step [30/91], Loss: 0.0005\n","Epoch [99/100], Step [40/91], Loss: 0.0004\n","Epoch [99/100], Step [50/91], Loss: 0.0005\n","Epoch [99/100], Step [60/91], Loss: 0.0007\n","Epoch [99/100], Step [70/91], Loss: 0.0004\n","Epoch [99/100], Step [80/91], Loss: 0.0007\n","Epoch [99/100], Step [90/91], Loss: 0.0010\n","Epoch [100/100], Step [10/91], Loss: 0.0009\n","Epoch [100/100], Step [20/91], Loss: 0.0006\n","Epoch [100/100], Step [30/91], Loss: 0.0005\n","Epoch [100/100], Step [40/91], Loss: 0.0007\n","Epoch [100/100], Step [50/91], Loss: 0.0004\n","Epoch [100/100], Step [60/91], Loss: 0.0006\n","Epoch [100/100], Step [70/91], Loss: 0.0009\n","Epoch [100/100], Step [80/91], Loss: 0.0005\n","Epoch [100/100], Step [90/91], Loss: 0.0005\n"]}],"source":["train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=N_EPOCHS, learning_rate=LEARNING_RATE)"]},{"cell_type":"markdown","metadata":{"id":"M4pLJ7S4fLOP"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iY6Bnn5uAe4c"},"outputs":[],"source":["def evaluate(test_loader,batch_size, n_features):\n","  with torch.no_grad():\n","      predictions = []\n","      values = []\n","\n","      for x_test, y_test in test_loader:\n","        batch_size = x_test.shape[0]\n","        x_test = x_test.view([batch_size, -1, n_features])\n","        y_preds = model(x_test)\n","        y_preds = [ j for i in y_preds.tolist() for j in i  ]\n","        y_test = [ j for i in y_test.tolist() for j in i  ]\n","        predictions.extend(y_preds)\n","        values.extend(y_test)\n","\n","  return predictions, values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M40P2X_LBWY2"},"outputs":[],"source":["y_test_preds, y_test = evaluate(test_loader, BATCH_SIZE, N_FEATURES)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-TjsVyQJwXr"},"outputs":[],"source":["def inverse_transform(y, n_features):\n","  # Scaling back the predictions\n","  # some hacks to make inverse_transform work \n","  # create empty table with n_cols fields\n","  data_like = np.zeros(shape=(len(y), n_features))\n","  # put the predicted values in the right field\n","  data_like[:, 0] = y # assuming aqi values are always in first column \n","  # inverse transform and then select the right field\n","  return scaler.inverse_transform(data_like)[:,0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8tQaA9_KISf"},"outputs":[],"source":["y_test_preds = inverse_transform(y_test_preds, 5) # HERE n_features = # of SCALED features \n","y_test = inverse_transform(y_test, 5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lgAiPh0zfMBM"},"outputs":[],"source":["def calculate_metrics(pred, actual, verbose=True):\n","    result_metrics = {'mae' : mean_absolute_error(pred, actual),\n","                      'mape' : mean_absolute_percentage_error(pred, actual),\n","                      'mse' : mean_squared_error(pred, actual), \n","                      'rmse' : mean_squared_error(pred, actual) ** 0.5\n","                      }\n","    \n","    if verbose:\n","      print(\"Mean Absolute Error:       \", result_metrics[\"mae\"])\n","      print(\"Mean Absolute Percentage Error:       \", result_metrics[\"mape\"])\n","      print(\"Mean Squared Error:   \", result_metrics[\"mse\"])\n","      print(\"Root Mean Squared Error:   \", result_metrics[\"rmse\"])\n","      \n","    return result_metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"executionInfo":{"elapsed":231,"status":"error","timestamp":1670226773190,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"pcuiMgmsCXgB","outputId":"dcfb2395-a7cd-46ba-b1d7-e54b2df8a72e"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-a926e195ed62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'y_test_preds' is not defined"]}],"source":["metrics = calculate_metrics(y_test_preds, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"elapsed":4,"status":"error","timestamp":1670226774092,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"uFPW8EUhToFh","outputId":"aa512be8-9bcd-4026-e779-e8d369f4945b"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-c343ec514daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Actual'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1440x360 with 0 Axes>"]},"metadata":{}}],"source":["test_times = aqi_daily.iloc[2922+TIMESTEPS:,0]\n","\n","plt.figure(figsize=(20,5))\n","plt.plot(test_times, y_test, color = 'red', linewidth=2.0, alpha = 0.6)\n","plt.plot(test_times, y_test_preds, color = 'blue', linewidth=0.8)\n","plt.legend(['Actual','Predicted'])\n","plt.xlabel('Time')\n","plt.ylabel('AQI values')\n","plt.title('Test data prediction')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"lQ4vhQM7KvvH"},"source":["# Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OMNgReMaQE9U"},"outputs":[],"source":["N_FEATURES = train.shape[1]\n","INPUT_DIM = N_FEATURES\n","OUTPUT_DIM = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEIIp8WxKuVN"},"outputs":[],"source":["# GRU Hyperparameters\n","TIMESTEPS = [1, 3, 5, 7, 10]\n","LEARNING_RATE = [0.001, 0.1]\n","BATCH_SIZE = [32, 64, 128]\n","N_EPOCHS = [10, 50, 100]\n","DROPOUT = [0, 0.1, 0.5, 0.7]\n","WEIGHT_DECAY = [0] # TODO: research reasonable values \n","HIDDEN_DIM = [32, 128] \n","LAYER_DIM = [2]"]},{"cell_type":"markdown","metadata":{"id":"av4sePl3U48v"},"source":["## GRU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DvygKipJMi4S"},"outputs":[],"source":["# For loops to implement random search of hyperparameters \n","criterion = torch.nn.MSELoss()\n","errors_df = pd.DataFrame()\n","\n","for t in TIMESTEPS:\n","  train_ts = TimeSeries(train, t, N_FEATURES)\n","  test_ts = TimeSeries(test, t, N_FEATURES) \n","\n","  for b in BATCH_SIZE:\n","    train_loader = DataLoader(train_ts, shuffle=True, batch_size=b)\n","    test_loader = DataLoader(test_ts, shuffle=True, batch_size=b)\n","\n","    for l in LEARNING_RATE: \n","      for e in N_EPOCHS:\n","        for d in DROPOUT:\n","          for w in WEIGHT_DECAY:\n","            for h in HIDDEN_DIM:\n","              for lyr in LAYER_DIM: \n","                model = GRU(INPUT_DIM, h, lyr, OUTPUT_DIM, d) \n","                optimizer = torch.optim.Adam(model.parameters(),lr=l, weight_decay=w)\n","                train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=e, learning_rate=l, verbose=False)\n","\n","                y_test_preds, y_test = evaluate(test_loader, b, N_FEATURES)\n","                y_test_preds = inverse_transform(y_test_preds, 5)\n","                y_test = inverse_transform(y_test, 5)\n","\n","                params = { 'timesteps': t, 'batch_size': b, 'learning_rate': l, 'epochs':e, 'dropout': d,\n","                          'weight_decay' : w, 'hidden_dim': h, 'layer_dim': lyr}\n","                metrics = calculate_metrics(y_test_preds, y_test, verbose=False)\n","                out_dict = {**params,  **metrics}\n","\n","                errors_df = errors_df.append(out_dict, ignore_index=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":979},"executionInfo":{"elapsed":157,"status":"ok","timestamp":1669852083034,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"},"user_tz":480},"id":"JnpYL926N4M6","outputId":"8aff6d8e-6c0a-49b8-e1c5-cefadd285204"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c27df264-6d97-4bab-8476-3c6a89c30af7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>timesteps</th>\n","      <th>batch_size</th>\n","      <th>learning_rate</th>\n","      <th>epochs</th>\n","      <th>dropout</th>\n","      <th>weight_decay</th>\n","      <th>hidden_dim</th>\n","      <th>layer_dim</th>\n","      <th>mae</th>\n","      <th>mape</th>\n","      <th>mse</th>\n","      <th>rmse</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>673</th>\n","      <td>10.0</td>\n","      <td>128.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.326026</td>\n","      <td>0.176161</td>\n","      <td>203.803264</td>\n","      <td>14.275968</td>\n","    </tr>\n","    <tr>\n","      <th>432</th>\n","      <td>7.0</td>\n","      <td>32.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.279652</td>\n","      <td>0.182132</td>\n","      <td>204.028190</td>\n","      <td>14.283844</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>3.0</td>\n","      <td>64.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.258016</td>\n","      <td>0.177891</td>\n","      <td>205.408382</td>\n","      <td>14.332075</td>\n","    </tr>\n","    <tr>\n","      <th>481</th>\n","      <td>7.0</td>\n","      <td>64.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.245228</td>\n","      <td>0.176793</td>\n","      <td>205.572090</td>\n","      <td>14.337785</td>\n","    </tr>\n","    <tr>\n","      <th>531</th>\n","      <td>7.0</td>\n","      <td>128.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.315992</td>\n","      <td>0.181059</td>\n","      <td>206.358541</td>\n","      <td>14.365185</td>\n","    </tr>\n","    <tr>\n","      <th>529</th>\n","      <td>7.0</td>\n","      <td>128.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.077035</td>\n","      <td>0.178338</td>\n","      <td>206.532612</td>\n","      <td>14.371243</td>\n","    </tr>\n","    <tr>\n","      <th>434</th>\n","      <td>7.0</td>\n","      <td>32.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.283469</td>\n","      <td>0.176611</td>\n","      <td>207.016733</td>\n","      <td>14.388076</td>\n","    </tr>\n","    <tr>\n","      <th>343</th>\n","      <td>5.0</td>\n","      <td>64.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.7</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.419908</td>\n","      <td>0.180651</td>\n","      <td>207.228932</td>\n","      <td>14.395448</td>\n","    </tr>\n","    <tr>\n","      <th>396</th>\n","      <td>5.0</td>\n","      <td>128.0</td>\n","      <td>0.001</td>\n","      <td>50.0</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.577021</td>\n","      <td>0.185705</td>\n","      <td>207.230190</td>\n","      <td>14.395492</td>\n","    </tr>\n","    <tr>\n","      <th>480</th>\n","      <td>7.0</td>\n","      <td>64.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.214243</td>\n","      <td>0.179561</td>\n","      <td>207.634384</td>\n","      <td>14.409524</td>\n","    </tr>\n","    <tr>\n","      <th>290</th>\n","      <td>5.0</td>\n","      <td>32.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.274879</td>\n","      <td>0.178995</td>\n","      <td>207.704777</td>\n","      <td>14.411966</td>\n","    </tr>\n","    <tr>\n","      <th>336</th>\n","      <td>5.0</td>\n","      <td>64.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.357876</td>\n","      <td>0.177914</td>\n","      <td>207.817415</td>\n","      <td>14.415874</td>\n","    </tr>\n","    <tr>\n","      <th>627</th>\n","      <td>10.0</td>\n","      <td>64.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.157131</td>\n","      <td>0.177220</td>\n","      <td>207.873825</td>\n","      <td>14.417830</td>\n","    </tr>\n","    <tr>\n","      <th>483</th>\n","      <td>7.0</td>\n","      <td>64.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.272372</td>\n","      <td>0.177593</td>\n","      <td>207.949623</td>\n","      <td>14.420458</td>\n","    </tr>\n","    <tr>\n","      <th>288</th>\n","      <td>5.0</td>\n","      <td>32.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.610763</td>\n","      <td>0.184057</td>\n","      <td>207.956928</td>\n","      <td>14.420712</td>\n","    </tr>\n","    <tr>\n","      <th>385</th>\n","      <td>5.0</td>\n","      <td>128.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.166602</td>\n","      <td>0.179386</td>\n","      <td>208.058782</td>\n","      <td>14.424243</td>\n","    </tr>\n","    <tr>\n","      <th>624</th>\n","      <td>10.0</td>\n","      <td>64.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.194269</td>\n","      <td>0.178386</td>\n","      <td>208.100832</td>\n","      <td>14.425700</td>\n","    </tr>\n","    <tr>\n","      <th>384</th>\n","      <td>5.0</td>\n","      <td>128.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.319705</td>\n","      <td>0.177227</td>\n","      <td>208.321832</td>\n","      <td>14.433358</td>\n","    </tr>\n","    <tr>\n","      <th>625</th>\n","      <td>10.0</td>\n","      <td>64.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.490873</td>\n","      <td>0.180204</td>\n","      <td>208.505450</td>\n","      <td>14.439718</td>\n","    </tr>\n","    <tr>\n","      <th>435</th>\n","      <td>7.0</td>\n","      <td>32.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.236812</td>\n","      <td>0.177953</td>\n","      <td>208.585989</td>\n","      <td>14.442506</td>\n","    </tr>\n","    <tr>\n","      <th>576</th>\n","      <td>10.0</td>\n","      <td>32.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.370376</td>\n","      <td>0.181154</td>\n","      <td>208.608687</td>\n","      <td>14.443292</td>\n","    </tr>\n","    <tr>\n","      <th>387</th>\n","      <td>5.0</td>\n","      <td>128.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.267417</td>\n","      <td>0.176815</td>\n","      <td>208.950865</td>\n","      <td>14.455133</td>\n","    </tr>\n","    <tr>\n","      <th>243</th>\n","      <td>3.0</td>\n","      <td>128.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.197882</td>\n","      <td>0.179169</td>\n","      <td>209.052772</td>\n","      <td>14.458657</td>\n","    </tr>\n","    <tr>\n","      <th>202</th>\n","      <td>3.0</td>\n","      <td>64.0</td>\n","      <td>0.001</td>\n","      <td>50.0</td>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.354761</td>\n","      <td>0.180581</td>\n","      <td>210.008447</td>\n","      <td>14.491668</td>\n","    </tr>\n","    <tr>\n","      <th>337</th>\n","      <td>5.0</td>\n","      <td>64.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.317115</td>\n","      <td>0.182514</td>\n","      <td>210.053891</td>\n","      <td>14.493236</td>\n","    </tr>\n","    <tr>\n","      <th>433</th>\n","      <td>7.0</td>\n","      <td>32.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.548991</td>\n","      <td>0.179906</td>\n","      <td>210.140048</td>\n","      <td>14.496208</td>\n","    </tr>\n","    <tr>\n","      <th>293</th>\n","      <td>5.0</td>\n","      <td>32.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.429475</td>\n","      <td>0.179930</td>\n","      <td>210.156588</td>\n","      <td>14.496779</td>\n","    </tr>\n","    <tr>\n","      <th>151</th>\n","      <td>3.0</td>\n","      <td>32.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.7</td>\n","      <td>0.0</td>\n","      <td>128.0</td>\n","      <td>2.0</td>\n","      <td>9.705796</td>\n","      <td>0.186895</td>\n","      <td>210.295047</td>\n","      <td>14.501553</td>\n","    </tr>\n","    <tr>\n","      <th>144</th>\n","      <td>3.0</td>\n","      <td>32.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.288040</td>\n","      <td>0.181073</td>\n","      <td>210.606908</td>\n","      <td>14.512302</td>\n","    </tr>\n","    <tr>\n","      <th>192</th>\n","      <td>3.0</td>\n","      <td>64.0</td>\n","      <td>0.001</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>32.0</td>\n","      <td>2.0</td>\n","      <td>9.310613</td>\n","      <td>0.182598</td>\n","      <td>210.779339</td>\n","      <td>14.518242</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c27df264-6d97-4bab-8476-3c6a89c30af7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c27df264-6d97-4bab-8476-3c6a89c30af7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c27df264-6d97-4bab-8476-3c6a89c30af7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["     timesteps  batch_size  learning_rate  epochs  dropout  weight_decay  \\\n","673       10.0       128.0          0.001    10.0      0.0           0.0   \n","432        7.0        32.0          0.001    10.0      0.0           0.0   \n","195        3.0        64.0          0.001    10.0      0.1           0.0   \n","481        7.0        64.0          0.001    10.0      0.0           0.0   \n","531        7.0       128.0          0.001    10.0      0.1           0.0   \n","529        7.0       128.0          0.001    10.0      0.0           0.0   \n","434        7.0        32.0          0.001    10.0      0.1           0.0   \n","343        5.0        64.0          0.001    10.0      0.7           0.0   \n","396        5.0       128.0          0.001    50.0      0.5           0.0   \n","480        7.0        64.0          0.001    10.0      0.0           0.0   \n","290        5.0        32.0          0.001    10.0      0.1           0.0   \n","336        5.0        64.0          0.001    10.0      0.0           0.0   \n","627       10.0        64.0          0.001    10.0      0.1           0.0   \n","483        7.0        64.0          0.001    10.0      0.1           0.0   \n","288        5.0        32.0          0.001    10.0      0.0           0.0   \n","385        5.0       128.0          0.001    10.0      0.0           0.0   \n","624       10.0        64.0          0.001    10.0      0.0           0.0   \n","384        5.0       128.0          0.001    10.0      0.0           0.0   \n","625       10.0        64.0          0.001    10.0      0.0           0.0   \n","435        7.0        32.0          0.001    10.0      0.1           0.0   \n","576       10.0        32.0          0.001    10.0      0.0           0.0   \n","387        5.0       128.0          0.001    10.0      0.1           0.0   \n","243        3.0       128.0          0.001    10.0      0.1           0.0   \n","202        3.0        64.0          0.001    50.0      0.1           0.0   \n","337        5.0        64.0          0.001    10.0      0.0           0.0   \n","433        7.0        32.0          0.001    10.0      0.0           0.0   \n","293        5.0        32.0          0.001    10.0      0.5           0.0   \n","151        3.0        32.0          0.001    10.0      0.7           0.0   \n","144        3.0        32.0          0.001    10.0      0.0           0.0   \n","192        3.0        64.0          0.001    10.0      0.0           0.0   \n","\n","     hidden_dim  layer_dim       mae      mape         mse       rmse  \n","673       128.0        2.0  9.326026  0.176161  203.803264  14.275968  \n","432        32.0        2.0  9.279652  0.182132  204.028190  14.283844  \n","195       128.0        2.0  9.258016  0.177891  205.408382  14.332075  \n","481       128.0        2.0  9.245228  0.176793  205.572090  14.337785  \n","531       128.0        2.0  9.315992  0.181059  206.358541  14.365185  \n","529       128.0        2.0  9.077035  0.178338  206.532612  14.371243  \n","434        32.0        2.0  9.283469  0.176611  207.016733  14.388076  \n","343       128.0        2.0  9.419908  0.180651  207.228932  14.395448  \n","396        32.0        2.0  9.577021  0.185705  207.230190  14.395492  \n","480        32.0        2.0  9.214243  0.179561  207.634384  14.409524  \n","290        32.0        2.0  9.274879  0.178995  207.704777  14.411966  \n","336        32.0        2.0  9.357876  0.177914  207.817415  14.415874  \n","627       128.0        2.0  9.157131  0.177220  207.873825  14.417830  \n","483       128.0        2.0  9.272372  0.177593  207.949623  14.420458  \n","288        32.0        2.0  9.610763  0.184057  207.956928  14.420712  \n","385       128.0        2.0  9.166602  0.179386  208.058782  14.424243  \n","624        32.0        2.0  9.194269  0.178386  208.100832  14.425700  \n","384        32.0        2.0  9.319705  0.177227  208.321832  14.433358  \n","625       128.0        2.0  9.490873  0.180204  208.505450  14.439718  \n","435       128.0        2.0  9.236812  0.177953  208.585989  14.442506  \n","576        32.0        2.0  9.370376  0.181154  208.608687  14.443292  \n","387       128.0        2.0  9.267417  0.176815  208.950865  14.455133  \n","243       128.0        2.0  9.197882  0.179169  209.052772  14.458657  \n","202        32.0        2.0  9.354761  0.180581  210.008447  14.491668  \n","337       128.0        2.0  9.317115  0.182514  210.053891  14.493236  \n","433       128.0        2.0  9.548991  0.179906  210.140048  14.496208  \n","293       128.0        2.0  9.429475  0.179930  210.156588  14.496779  \n","151       128.0        2.0  9.705796  0.186895  210.295047  14.501553  \n","144        32.0        2.0  9.288040  0.181073  210.606908  14.512302  \n","192        32.0        2.0  9.310613  0.182598  210.779339  14.518242  "]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["errors_df = errors_df.sort_values(by=\"rmse\")\n","errors_df.to_csv('GRU_daily_metrics_ss.csv')\n","errors_df.head(30)"]},{"cell_type":"markdown","metadata":{"id":"5riF8LGbU75W"},"source":["## CNN-LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDkCmmbyWZ64"},"outputs":[],"source":["# CNN-LSTM Hyperparameters (same like GRU, except TIMESTEPS)\n","TIMESTEPS = [7]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fO-Obgv4RWhc"},"outputs":[],"source":["# Additional CNN-LSTM Hyperparameter\n","N_FILTERS = N_FEATURES "]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ECnS54dvejlr","executionInfo":{"status":"ok","timestamp":1670389910399,"user_tz":480,"elapsed":173,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}},"outputId":"3eaef95a-7ce8-4e0f-d0f0-cb3eec0ce5e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks/stats207\n"]}]},{"cell_type":"code","source":["import csv\n","\n","output=open('metrics_output/CNNLSTM_daily_metrics_timestep7.csv', mode='a', newline='')\n","fieldnames = ['timesteps', 'batch_size', 'learning_rate', 'epochs', 'dropout', 'weight_decay', 'hidden_dim', 'layer_dim', 'window', 'mae', 'mape', 'mse', 'rmse']\n","output_writer = csv.DictWriter(output, fieldnames=fieldnames, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","output_writer.writeheader()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U3JlSHtFbPfm","executionInfo":{"status":"ok","timestamp":1670389961101,"user_tz":480,"elapsed":154,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}},"outputId":"fd52c611-17dc-49b4-b597-b46f9cd45e1a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["110"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5sjmawqWsS_","executionInfo":{"status":"error","timestamp":1670400099095,"user_tz":480,"elapsed":10137865,"user":{"displayName":"ALDA NAOMI","userId":"01061387583040321032"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"82adb524-1891-4598-c713-262274cb5f22"},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-53-66e94296c355>\", line 25, in <module>\n","    train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=e, learning_rate=l, verbose=False)\n","  File \"<ipython-input-35-c65644d1a3b3>\", line 18, in train_model\n","    optimizer.step()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\", line 113, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\", line 157, in step\n","    adam(params_with_grad,\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\", line 213, in adam\n","    func(params,\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\", line 305, in _single_tensor_adam\n","    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.8/inspect.py\", line 1477, in getframeinfo\n","    lines, lnum = findsource(frame)\n","  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 185, in findsource\n","    lines = linecache.getlines(file, globals_dict)\n","  File \"/usr/lib/python3.8/linecache.py\", line 47, in getlines\n","    return updatecache(filename, module_globals)\n","  File \"/usr/lib/python3.8/linecache.py\", line 136, in updatecache\n","    with tokenize.open(fullname) as fp:\n","  File \"/usr/lib/python3.8/tokenize.py\", line 394, in open\n","    encoding, lines = detect_encoding(buffer.readline)\n","  File \"/usr/lib/python3.8/tokenize.py\", line 363, in detect_encoding\n","    first = read_or_stop()\n","  File \"/usr/lib/python3.8/tokenize.py\", line 321, in read_or_stop\n","    return readline()\n","KeyboardInterrupt\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}],"source":["criterion = torch.nn.MSELoss()\n","errors_df = pd.DataFrame()\n","\n","for t in TIMESTEPS:\n","  train_ts = TimeSeries(train, t, N_FEATURES)\n","  test_ts = TimeSeries(test, t, N_FEATURES) \n","  WINDOW = [ i for i in range(2, t)]\n","  for b in BATCH_SIZE:\n","    train_loader = DataLoader(train_ts, shuffle=True, batch_size=b)\n","    test_loader = DataLoader(test_ts, shuffle=True, batch_size=b)\n","\n","    for l in LEARNING_RATE: \n","      for e in N_EPOCHS:\n","        for d in DROPOUT:\n","          for w in WEIGHT_DECAY:\n","            for h in HIDDEN_DIM:\n","              for lyr in LAYER_DIM: \n","                for win in WINDOW:\n","                  params = { 'timesteps': t, 'batch_size': b, 'learning_rate': l, 'epochs':e, 'dropout': d,\n","                            'weight_decay' : w, 'hidden_dim': h, 'layer_dim': lyr, 'window': win}\n","                  #print(params)\n","\n","                  model = CNNLSTM(INPUT_DIM, h, lyr, OUTPUT_DIM, d, win, N_FILTERS)\n","                  optimizer = torch.optim.Adam(model.parameters(),lr=l, weight_decay=w)\n","                  train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=e, learning_rate=l, verbose=False)\n","\n","                  y_test_preds, y_test = evaluate(test_loader, b, N_FEATURES)\n","                  y_test_preds = inverse_transform(y_test_preds, 5) # here N_FEATURES = 5 \n","                  y_test = inverse_transform(y_test, 5) # here N_FEATURES = 5 \n","\n","                  \n","                  metrics = calculate_metrics(y_test_preds, y_test, verbose=False)\n","                  out_dict = {**params,  **metrics}\n","\n","                  output_writer.writerow(out_dict)\n","                  output.flush()\n","                  errors_df = errors_df.append(out_dict, ignore_index=True)\n"]},{"cell_type":"code","source":["output.close()"],"metadata":{"id":"P4uN9wJCcEsZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yontbRBngwrx"},"outputs":[],"source":["errors_df = errors_df.sort_values(by=\"rmse\")\n","errors_df.to_csv('CNNLSTM_daily_metrics.csv')\n","errors_df.head(30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DUPOUDjQ-dBh"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["Jndg53XDdDmr","EOxahcgLaF4i","lopkaN1Yd2H3","gpA7ZhalfM-k","av4sePl3U48v"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}